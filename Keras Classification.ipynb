{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Vinay\\Documents\\Refactored_Py_DS_ML_Bootcamp-master\\TensorFlow_FILES\\DATA\\cancer_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>benign_0__mal_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  benign_0__mal_1  \n",
       "0          0.4601                  0.11890                0  \n",
       "1          0.2750                  0.08902                0  \n",
       "2          0.3613                  0.08758                0  \n",
       "3          0.6638                  0.17300                0  \n",
       "4          0.2364                  0.07678                0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  benign_0__mal_1          569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>28.11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>39.28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>188.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>2501.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.16340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.34540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.42680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.20120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.30400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>2.87300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>4.88500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>21.98000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>542.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.03113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.39600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.05279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.07895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.02984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>36.04000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>49.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>251.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>4254.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.22260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>1.05800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>1.25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.29100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.66380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benign_0__mal_1</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.627417</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count        mean         std         min  \\\n",
       "mean radius              569.0   14.127292    3.524049    6.981000   \n",
       "mean texture             569.0   19.289649    4.301036    9.710000   \n",
       "mean perimeter           569.0   91.969033   24.298981   43.790000   \n",
       "mean area                569.0  654.889104  351.914129  143.500000   \n",
       "mean smoothness          569.0    0.096360    0.014064    0.052630   \n",
       "mean compactness         569.0    0.104341    0.052813    0.019380   \n",
       "mean concavity           569.0    0.088799    0.079720    0.000000   \n",
       "mean concave points      569.0    0.048919    0.038803    0.000000   \n",
       "mean symmetry            569.0    0.181162    0.027414    0.106000   \n",
       "mean fractal dimension   569.0    0.062798    0.007060    0.049960   \n",
       "radius error             569.0    0.405172    0.277313    0.111500   \n",
       "texture error            569.0    1.216853    0.551648    0.360200   \n",
       "perimeter error          569.0    2.866059    2.021855    0.757000   \n",
       "area error               569.0   40.337079   45.491006    6.802000   \n",
       "smoothness error         569.0    0.007041    0.003003    0.001713   \n",
       "compactness error        569.0    0.025478    0.017908    0.002252   \n",
       "concavity error          569.0    0.031894    0.030186    0.000000   \n",
       "concave points error     569.0    0.011796    0.006170    0.000000   \n",
       "symmetry error           569.0    0.020542    0.008266    0.007882   \n",
       "fractal dimension error  569.0    0.003795    0.002646    0.000895   \n",
       "worst radius             569.0   16.269190    4.833242    7.930000   \n",
       "worst texture            569.0   25.677223    6.146258   12.020000   \n",
       "worst perimeter          569.0  107.261213   33.602542   50.410000   \n",
       "worst area               569.0  880.583128  569.356993  185.200000   \n",
       "worst smoothness         569.0    0.132369    0.022832    0.071170   \n",
       "worst compactness        569.0    0.254265    0.157336    0.027290   \n",
       "worst concavity          569.0    0.272188    0.208624    0.000000   \n",
       "worst concave points     569.0    0.114606    0.065732    0.000000   \n",
       "worst symmetry           569.0    0.290076    0.061867    0.156500   \n",
       "worst fractal dimension  569.0    0.083946    0.018061    0.055040   \n",
       "benign_0__mal_1          569.0    0.627417    0.483918    0.000000   \n",
       "\n",
       "                                25%         50%          75%         max  \n",
       "mean radius               11.700000   13.370000    15.780000    28.11000  \n",
       "mean texture              16.170000   18.840000    21.800000    39.28000  \n",
       "mean perimeter            75.170000   86.240000   104.100000   188.50000  \n",
       "mean area                420.300000  551.100000   782.700000  2501.00000  \n",
       "mean smoothness            0.086370    0.095870     0.105300     0.16340  \n",
       "mean compactness           0.064920    0.092630     0.130400     0.34540  \n",
       "mean concavity             0.029560    0.061540     0.130700     0.42680  \n",
       "mean concave points        0.020310    0.033500     0.074000     0.20120  \n",
       "mean symmetry              0.161900    0.179200     0.195700     0.30400  \n",
       "mean fractal dimension     0.057700    0.061540     0.066120     0.09744  \n",
       "radius error               0.232400    0.324200     0.478900     2.87300  \n",
       "texture error              0.833900    1.108000     1.474000     4.88500  \n",
       "perimeter error            1.606000    2.287000     3.357000    21.98000  \n",
       "area error                17.850000   24.530000    45.190000   542.20000  \n",
       "smoothness error           0.005169    0.006380     0.008146     0.03113  \n",
       "compactness error          0.013080    0.020450     0.032450     0.13540  \n",
       "concavity error            0.015090    0.025890     0.042050     0.39600  \n",
       "concave points error       0.007638    0.010930     0.014710     0.05279  \n",
       "symmetry error             0.015160    0.018730     0.023480     0.07895  \n",
       "fractal dimension error    0.002248    0.003187     0.004558     0.02984  \n",
       "worst radius              13.010000   14.970000    18.790000    36.04000  \n",
       "worst texture             21.080000   25.410000    29.720000    49.54000  \n",
       "worst perimeter           84.110000   97.660000   125.400000   251.20000  \n",
       "worst area               515.300000  686.500000  1084.000000  4254.00000  \n",
       "worst smoothness           0.116600    0.131300     0.146000     0.22260  \n",
       "worst compactness          0.147200    0.211900     0.339100     1.05800  \n",
       "worst concavity            0.114500    0.226700     0.382900     1.25200  \n",
       "worst concave points       0.064930    0.099930     0.161400     0.29100  \n",
       "worst symmetry             0.250400    0.282200     0.317900     0.66380  \n",
       "worst fractal dimension    0.071460    0.080040     0.092080     0.20750  \n",
       "benign_0__mal_1            0.000000    1.000000     1.000000     1.00000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-2951decb5341>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'benign_0__mal_1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    }
   ],
   "source": [
    "sns.countplot(x = 'benign_0__mal_1', data= df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x205a21d7e88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.corr()['benign_0__mal_1'][:-1].sort_values().plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('benign_0__mal_1', axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426, 30)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(15, activation=\"relu\"))\n",
    "\n",
    "#Binary Classification\n",
    "model.add(Dense(1, activation= 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      " 32/426 [=>............................] - ETA: 42s"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": " Blas GEMM launch failed : a.shape=(32, 30), b.shape=(30, 30), m=32, n=30, k=30\n\t [[node sequential/dense/MatMul (defined at <ipython-input-20-c04790621ab6>:1) ]] [Op:__inference_distributed_function_786]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c04790621ab6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m600\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m:  Blas GEMM launch failed : a.shape=(32, 30), b.shape=(30, 30), m=32, n=30, k=30\n\t [[node sequential/dense/MatMul (defined at <ipython-input-20-c04790621ab6>:1) ]] [Op:__inference_distributed_function_786]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "model.fit(x= X_train, y = y_train, epochs = 600, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 14s 33ms/sample - loss: 0.7214 - val_loss: 0.6761\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.6622 - val_loss: 0.6401\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.6274 - val_loss: 0.6087\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 214us/sample - loss: 0.5915 - val_loss: 0.5720\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.5547 - val_loss: 0.5333\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.5168 - val_loss: 0.4925\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.4731 - val_loss: 0.4463\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 203us/sample - loss: 0.4338 - val_loss: 0.4048\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.3917 - val_loss: 0.3627\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.3529 - val_loss: 0.3254\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 191us/sample - loss: 0.3195 - val_loss: 0.2925\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 220us/sample - loss: 0.2904 - val_loss: 0.2640\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 194us/sample - loss: 0.2670 - val_loss: 0.2450\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.2454 - val_loss: 0.2232\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.2296 - val_loss: 0.2111\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.2107 - val_loss: 0.1946\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.1983 - val_loss: 0.1850\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.1859 - val_loss: 0.1763\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.1802 - val_loss: 0.1666\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.1662 - val_loss: 0.1585\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 279us/sample - loss: 0.1567 - val_loss: 0.1570\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 0.1487 - val_loss: 0.1472\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 209us/sample - loss: 0.1409 - val_loss: 0.1459\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.1333 - val_loss: 0.1407\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 198us/sample - loss: 0.1348 - val_loss: 0.1378\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.1314 - val_loss: 0.1315\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.1224 - val_loss: 0.1324\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.1140 - val_loss: 0.1294\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 319us/sample - loss: 0.1075 - val_loss: 0.1262\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.1054 - val_loss: 0.1236\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0986 - val_loss: 0.1201\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0952 - val_loss: 0.1200\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0948 - val_loss: 0.1185\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0901 - val_loss: 0.1206\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0937 - val_loss: 0.1176\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.1056 - val_loss: 0.1182\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0892 - val_loss: 0.1119\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 277us/sample - loss: 0.0829 - val_loss: 0.1206\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 270us/sample - loss: 0.0804 - val_loss: 0.1147\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0775 - val_loss: 0.1116\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0762 - val_loss: 0.1144\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.0747 - val_loss: 0.1150\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0738 - val_loss: 0.1148\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.0717 - val_loss: 0.1108\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0713 - val_loss: 0.1132\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0719 - val_loss: 0.1118\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0696 - val_loss: 0.1112\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 219us/sample - loss: 0.0690 - val_loss: 0.1158\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 0.0680 - val_loss: 0.1130\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 389us/sample - loss: 0.0667 - val_loss: 0.1131\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 324us/sample - loss: 0.0656 - val_loss: 0.1161\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 333us/sample - loss: 0.0657 - val_loss: 0.1170\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 319us/sample - loss: 0.0656 - val_loss: 0.1116\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 345us/sample - loss: 0.0688 - val_loss: 0.1196\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 0.0662 - val_loss: 0.1125\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.0618 - val_loss: 0.1167\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0618 - val_loss: 0.1122\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 200us/sample - loss: 0.0611 - val_loss: 0.1175\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0639 - val_loss: 0.1147\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0623 - val_loss: 0.1169\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0609 - val_loss: 0.1101\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0597 - val_loss: 0.1249\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0600 - val_loss: 0.1136\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0579 - val_loss: 0.1168\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.0583 - val_loss: 0.1144\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 219us/sample - loss: 0.0584 - val_loss: 0.1168\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0565 - val_loss: 0.1230\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0590 - val_loss: 0.1165\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0555 - val_loss: 0.1179\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0553 - val_loss: 0.1158\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0558 - val_loss: 0.1150\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0555 - val_loss: 0.1212\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0657 - val_loss: 0.1155\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 227us/sample - loss: 0.0602 - val_loss: 0.1220\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 271us/sample - loss: 0.0629 - val_loss: 0.1229\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0559 - val_loss: 0.1212\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0562 - val_loss: 0.1182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0567 - val_loss: 0.1189\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 214us/sample - loss: 0.0544 - val_loss: 0.1217\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 214us/sample - loss: 0.0540 - val_loss: 0.1191\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 312us/sample - loss: 0.0529 - val_loss: 0.1192\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 277us/sample - loss: 0.0518 - val_loss: 0.1241\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 293us/sample - loss: 0.0519 - val_loss: 0.1215\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 241us/sample - loss: 0.0514 - val_loss: 0.1209\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0510 - val_loss: 0.1283\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0527 - val_loss: 0.1242\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0515 - val_loss: 0.1259\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0523 - val_loss: 0.1223\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0520 - val_loss: 0.1246\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 231us/sample - loss: 0.0501 - val_loss: 0.1269\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 250us/sample - loss: 0.0508 - val_loss: 0.1191\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0499 - val_loss: 0.1281\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 231us/sample - loss: 0.0502 - val_loss: 0.1210\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 233us/sample - loss: 0.0514 - val_loss: 0.1318\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0550 - val_loss: 0.1236\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0509 - val_loss: 0.1229\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.0516 - val_loss: 0.1285\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 267us/sample - loss: 0.0551 - val_loss: 0.1236\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 296us/sample - loss: 0.0494 - val_loss: 0.1298\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 276us/sample - loss: 0.0485 - val_loss: 0.1234\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 323us/sample - loss: 0.0497 - val_loss: 0.1247\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 306us/sample - loss: 0.0522 - val_loss: 0.1332\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 352us/sample - loss: 0.0480 - val_loss: 0.1226\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.0477 - val_loss: 0.1284\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 298us/sample - loss: 0.0486 - val_loss: 0.1236\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 238us/sample - loss: 0.0471 - val_loss: 0.1294\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.0478 - val_loss: 0.1287\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0464 - val_loss: 0.1271\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 254us/sample - loss: 0.0472 - val_loss: 0.1366\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 219us/sample - loss: 0.0483 - val_loss: 0.1292\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0463 - val_loss: 0.1274\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0465 - val_loss: 0.1265\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0477 - val_loss: 0.1317\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0465 - val_loss: 0.1238\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0447 - val_loss: 0.1331\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0478 - val_loss: 0.1289\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 200us/sample - loss: 0.0553 - val_loss: 0.1338\n",
      "Epoch 118/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0510 - val_loss: 0.1277\n",
      "Epoch 119/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0479 - val_loss: 0.1351\n",
      "Epoch 120/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0447 - val_loss: 0.1296\n",
      "Epoch 121/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.0466 - val_loss: 0.1296\n",
      "Epoch 122/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0443 - val_loss: 0.1399\n",
      "Epoch 123/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.0449 - val_loss: 0.1319\n",
      "Epoch 124/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0442 - val_loss: 0.1325\n",
      "Epoch 125/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0454 - val_loss: 0.1316\n",
      "Epoch 126/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.0488 - val_loss: 0.1481\n",
      "Epoch 127/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0447 - val_loss: 0.1261\n",
      "Epoch 128/600\n",
      "426/426 [==============================] - 0s 162us/sample - loss: 0.0475 - val_loss: 0.1401\n",
      "Epoch 129/600\n",
      "426/426 [==============================] - 0s 222us/sample - loss: 0.0431 - val_loss: 0.1300\n",
      "Epoch 130/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0451 - val_loss: 0.1329\n",
      "Epoch 131/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0508 - val_loss: 0.1355\n",
      "Epoch 132/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.0462 - val_loss: 0.1285\n",
      "Epoch 133/600\n",
      "426/426 [==============================] - 0s 196us/sample - loss: 0.0428 - val_loss: 0.1377\n",
      "Epoch 134/600\n",
      "426/426 [==============================] - 0s 177us/sample - loss: 0.0425 - val_loss: 0.1402\n",
      "Epoch 135/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.0427 - val_loss: 0.1340\n",
      "Epoch 136/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0481 - val_loss: 0.1361\n",
      "Epoch 137/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0436 - val_loss: 0.1307\n",
      "Epoch 138/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0469 - val_loss: 0.1371\n",
      "Epoch 139/600\n",
      "426/426 [==============================] - 0s 198us/sample - loss: 0.0448 - val_loss: 0.1296\n",
      "Epoch 140/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0454 - val_loss: 0.1399\n",
      "Epoch 141/600\n",
      "426/426 [==============================] - 0s 206us/sample - loss: 0.0419 - val_loss: 0.1271\n",
      "Epoch 142/600\n",
      "426/426 [==============================] - 0s 209us/sample - loss: 0.0426 - val_loss: 0.1356\n",
      "Epoch 143/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.0450 - val_loss: 0.1331\n",
      "Epoch 144/600\n",
      "426/426 [==============================] - 0s 150us/sample - loss: 0.0515 - val_loss: 0.1366\n",
      "Epoch 145/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0422 - val_loss: 0.1305\n",
      "Epoch 146/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.0418 - val_loss: 0.1357\n",
      "Epoch 147/600\n",
      "426/426 [==============================] - 0s 215us/sample - loss: 0.0408 - val_loss: 0.1306\n",
      "Epoch 148/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.0428 - val_loss: 0.1360\n",
      "Epoch 149/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0405 - val_loss: 0.1324\n",
      "Epoch 150/600\n",
      "426/426 [==============================] - 0s 190us/sample - loss: 0.0404 - val_loss: 0.1415\n",
      "Epoch 151/600\n",
      "426/426 [==============================] - 0s 199us/sample - loss: 0.0410 - val_loss: 0.1434\n",
      "Epoch 152/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0405 - val_loss: 0.1353\n",
      "Epoch 153/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.0412 - val_loss: 0.1378\n",
      "Epoch 154/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0402 - val_loss: 0.1403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.0394 - val_loss: 0.1376\n",
      "Epoch 156/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.0420 - val_loss: 0.1344\n",
      "Epoch 157/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0398 - val_loss: 0.1473\n",
      "Epoch 158/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0425 - val_loss: 0.1413\n",
      "Epoch 159/600\n",
      "426/426 [==============================] - 0s 320us/sample - loss: 0.0397 - val_loss: 0.1433\n",
      "Epoch 160/600\n",
      "426/426 [==============================] - 0s 333us/sample - loss: 0.0424 - val_loss: 0.1456\n",
      "Epoch 161/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.0393 - val_loss: 0.1370\n",
      "Epoch 162/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.0400 - val_loss: 0.1388\n",
      "Epoch 163/600\n",
      "426/426 [==============================] - 0s 310us/sample - loss: 0.0386 - val_loss: 0.1440\n",
      "Epoch 164/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.0438 - val_loss: 0.1415\n",
      "Epoch 165/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0408 - val_loss: 0.1424\n",
      "Epoch 166/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0413 - val_loss: 0.1336\n",
      "Epoch 167/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0376 - val_loss: 0.1510\n",
      "Epoch 168/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0382 - val_loss: 0.1352\n",
      "Epoch 169/600\n",
      "426/426 [==============================] - 0s 270us/sample - loss: 0.0380 - val_loss: 0.1466\n",
      "Epoch 170/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0398 - val_loss: 0.1379\n",
      "Epoch 171/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0383 - val_loss: 0.1393\n",
      "Epoch 172/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0378 - val_loss: 0.1431\n",
      "Epoch 173/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0373 - val_loss: 0.1442\n",
      "Epoch 174/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0375 - val_loss: 0.1455\n",
      "Epoch 175/600\n",
      "426/426 [==============================] - 0s 263us/sample - loss: 0.0379 - val_loss: 0.1412\n",
      "Epoch 176/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.0376 - val_loss: 0.1366\n",
      "Epoch 177/600\n",
      "426/426 [==============================] - 0s 270us/sample - loss: 0.0371 - val_loss: 0.1448\n",
      "Epoch 178/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0365 - val_loss: 0.1459\n",
      "Epoch 179/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0371 - val_loss: 0.1433\n",
      "Epoch 180/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0379 - val_loss: 0.1553\n",
      "Epoch 181/600\n",
      "426/426 [==============================] - 0s 263us/sample - loss: 0.0424 - val_loss: 0.1504\n",
      "Epoch 182/600\n",
      "426/426 [==============================] - 0s 277us/sample - loss: 0.0374 - val_loss: 0.1435\n",
      "Epoch 183/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0354 - val_loss: 0.1335\n",
      "Epoch 184/600\n",
      "426/426 [==============================] - 0s 263us/sample - loss: 0.0387 - val_loss: 0.1462\n",
      "Epoch 185/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.0359 - val_loss: 0.1414\n",
      "Epoch 186/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0361 - val_loss: 0.1466\n",
      "Epoch 187/600\n",
      "426/426 [==============================] - 0s 199us/sample - loss: 0.0363 - val_loss: 0.1523\n",
      "Epoch 188/600\n",
      "426/426 [==============================] - 0s 196us/sample - loss: 0.0354 - val_loss: 0.1364\n",
      "Epoch 189/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0387 - val_loss: 0.1466\n",
      "Epoch 190/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0357 - val_loss: 0.1465\n",
      "Epoch 191/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0359 - val_loss: 0.1453\n",
      "Epoch 192/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0352 - val_loss: 0.1437\n",
      "Epoch 193/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0379 - val_loss: 0.1525\n",
      "Epoch 194/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0353 - val_loss: 0.1471\n",
      "Epoch 195/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0340 - val_loss: 0.1495\n",
      "Epoch 196/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0336 - val_loss: 0.1428\n",
      "Epoch 197/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0341 - val_loss: 0.1435\n",
      "Epoch 198/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.0348 - val_loss: 0.1437\n",
      "Epoch 199/600\n",
      "426/426 [==============================] - 0s 262us/sample - loss: 0.0332 - val_loss: 0.1455\n",
      "Epoch 200/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0342 - val_loss: 0.1448\n",
      "Epoch 201/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0329 - val_loss: 0.1411\n",
      "Epoch 202/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0343 - val_loss: 0.1592\n",
      "Epoch 203/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0375 - val_loss: 0.1382\n",
      "Epoch 204/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0350 - val_loss: 0.1406\n",
      "Epoch 205/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0336 - val_loss: 0.1403\n",
      "Epoch 206/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0355 - val_loss: 0.1489\n",
      "Epoch 207/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.0342 - val_loss: 0.1384\n",
      "Epoch 208/600\n",
      "426/426 [==============================] - 0s 290us/sample - loss: 0.0338 - val_loss: 0.1507\n",
      "Epoch 209/600\n",
      "426/426 [==============================] - 0s 303us/sample - loss: 0.0357 - val_loss: 0.1471\n",
      "Epoch 210/600\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.044 - 0s 408us/sample - loss: 0.0418 - val_loss: 0.1382\n",
      "Epoch 211/600\n",
      "426/426 [==============================] - 0s 382us/sample - loss: 0.0498 - val_loss: 0.1359\n",
      "Epoch 212/600\n",
      "426/426 [==============================] - 0s 371us/sample - loss: 0.0370 - val_loss: 0.1393\n",
      "Epoch 213/600\n",
      "426/426 [==============================] - 0s 312us/sample - loss: 0.0415 - val_loss: 0.1461\n",
      "Epoch 214/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 0.0363 - val_loss: 0.1419\n",
      "Epoch 215/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0350 - val_loss: 0.1438\n",
      "Epoch 216/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0341 - val_loss: 0.1456\n",
      "Epoch 217/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0332 - val_loss: 0.1518\n",
      "Epoch 218/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0319 - val_loss: 0.1487\n",
      "Epoch 219/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0325 - val_loss: 0.1448\n",
      "Epoch 220/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0310 - val_loss: 0.1491\n",
      "Epoch 221/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0374 - val_loss: 0.1468\n",
      "Epoch 222/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0352 - val_loss: 0.1419\n",
      "Epoch 223/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0369 - val_loss: 0.1585\n",
      "Epoch 224/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0408 - val_loss: 0.1299\n",
      "Epoch 225/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0402 - val_loss: 0.1637\n",
      "Epoch 226/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.0338 - val_loss: 0.1399\n",
      "Epoch 227/600\n",
      "426/426 [==============================] - 0s 208us/sample - loss: 0.0346 - val_loss: 0.1459\n",
      "Epoch 228/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0345 - val_loss: 0.1474\n",
      "Epoch 229/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0295 - val_loss: 0.1431\n",
      "Epoch 230/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0304 - val_loss: 0.1440\n",
      "Epoch 231/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0300 - val_loss: 0.1548\n",
      "Epoch 232/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0301 - val_loss: 0.1416\n",
      "Epoch 233/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0302 - val_loss: 0.1557\n",
      "Epoch 234/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.0354 - val_loss: 0.1348\n",
      "Epoch 235/600\n",
      "426/426 [==============================] - 0s 233us/sample - loss: 0.0317 - val_loss: 0.1479\n",
      "Epoch 236/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0290 - val_loss: 0.1399\n",
      "Epoch 237/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0308 - val_loss: 0.1486\n",
      "Epoch 238/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.0323 - val_loss: 0.1498\n",
      "Epoch 239/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0297 - val_loss: 0.1467\n",
      "Epoch 240/600\n",
      "426/426 [==============================] - 0s 277us/sample - loss: 0.0340 - val_loss: 0.1550\n",
      "Epoch 241/600\n",
      "426/426 [==============================] - 0s 293us/sample - loss: 0.0359 - val_loss: 0.1346\n",
      "Epoch 242/600\n",
      "426/426 [==============================] - 0s 324us/sample - loss: 0.0325 - val_loss: 0.1536\n",
      "Epoch 243/600\n",
      "426/426 [==============================] - 0s 277us/sample - loss: 0.0294 - val_loss: 0.1461\n",
      "Epoch 244/600\n",
      "426/426 [==============================] - 0s 267us/sample - loss: 0.0328 - val_loss: 0.1459\n",
      "Epoch 245/600\n",
      "426/426 [==============================] - 0s 270us/sample - loss: 0.0365 - val_loss: 0.1465\n",
      "Epoch 246/600\n",
      "426/426 [==============================] - 0s 279us/sample - loss: 0.0316 - val_loss: 0.1515\n",
      "Epoch 247/600\n",
      "426/426 [==============================] - 0s 298us/sample - loss: 0.0276 - val_loss: 0.1433\n",
      "Epoch 248/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0282 - val_loss: 0.1482\n",
      "Epoch 249/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0299 - val_loss: 0.1373\n",
      "Epoch 250/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0330 - val_loss: 0.1592\n",
      "Epoch 251/600\n",
      "426/426 [==============================] - 0s 275us/sample - loss: 0.0285 - val_loss: 0.1415\n",
      "Epoch 252/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.0282 - val_loss: 0.1498\n",
      "Epoch 253/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0268 - val_loss: 0.1484\n",
      "Epoch 254/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.0267 - val_loss: 0.1422\n",
      "Epoch 255/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0277 - val_loss: 0.1446\n",
      "Epoch 256/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0276 - val_loss: 0.1526\n",
      "Epoch 257/600\n",
      "426/426 [==============================] - 0s 184us/sample - loss: 0.0269 - val_loss: 0.1492\n",
      "Epoch 258/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0323 - val_loss: 0.1581\n",
      "Epoch 259/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0297 - val_loss: 0.1371\n",
      "Epoch 260/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0302 - val_loss: 0.1687\n",
      "Epoch 261/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0287 - val_loss: 0.1462\n",
      "Epoch 262/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0258 - val_loss: 0.1556\n",
      "Epoch 263/600\n",
      "426/426 [==============================] - 0s 311us/sample - loss: 0.0260 - val_loss: 0.1450\n",
      "Epoch 264/600\n",
      "426/426 [==============================] - 0s 352us/sample - loss: 0.0297 - val_loss: 0.1762\n",
      "Epoch 265/600\n",
      "426/426 [==============================] - 0s 310us/sample - loss: 0.0306 - val_loss: 0.1454\n",
      "Epoch 266/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.0261 - val_loss: 0.1567\n",
      "Epoch 267/600\n",
      "426/426 [==============================] - 0s 263us/sample - loss: 0.0258 - val_loss: 0.1462\n",
      "Epoch 268/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0279 - val_loss: 0.1520\n",
      "Epoch 269/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0267 - val_loss: 0.1512\n",
      "Epoch 270/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0252 - val_loss: 0.1469\n",
      "Epoch 271/600\n",
      "426/426 [==============================] - 0s 212us/sample - loss: 0.0248 - val_loss: 0.1512\n",
      "Epoch 272/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0255 - val_loss: 0.1619\n",
      "Epoch 273/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0262 - val_loss: 0.1404\n",
      "Epoch 274/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0290 - val_loss: 0.1541\n",
      "Epoch 275/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0277 - val_loss: 0.1492\n",
      "Epoch 276/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0261 - val_loss: 0.1453\n",
      "Epoch 277/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0252 - val_loss: 0.1510\n",
      "Epoch 278/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0273 - val_loss: 0.1445\n",
      "Epoch 279/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0279 - val_loss: 0.1519\n",
      "Epoch 280/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.0241 - val_loss: 0.1559\n",
      "Epoch 281/600\n",
      "426/426 [==============================] - 0s 229us/sample - loss: 0.0239 - val_loss: 0.1552\n",
      "Epoch 282/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0237 - val_loss: 0.1620\n",
      "Epoch 283/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0249 - val_loss: 0.1610\n",
      "Epoch 284/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0307 - val_loss: 0.1550\n",
      "Epoch 285/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0279 - val_loss: 0.1599\n",
      "Epoch 286/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0245 - val_loss: 0.1511\n",
      "Epoch 287/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0238 - val_loss: 0.1587\n",
      "Epoch 288/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0250 - val_loss: 0.1616\n",
      "Epoch 289/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0267 - val_loss: 0.1690\n",
      "Epoch 290/600\n",
      "426/426 [==============================] - 0s 241us/sample - loss: 0.0249 - val_loss: 0.1477\n",
      "Epoch 291/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0247 - val_loss: 0.1603\n",
      "Epoch 292/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0225 - val_loss: 0.1508\n",
      "Epoch 293/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.0236 - val_loss: 0.1624\n",
      "Epoch 294/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0248 - val_loss: 0.1663\n",
      "Epoch 295/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.0234 - val_loss: 0.1581\n",
      "Epoch 296/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0232 - val_loss: 0.1607\n",
      "Epoch 297/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0219 - val_loss: 0.1549\n",
      "Epoch 298/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0221 - val_loss: 0.1623\n",
      "Epoch 299/600\n",
      "426/426 [==============================] - 0s 215us/sample - loss: 0.0215 - val_loss: 0.1547\n",
      "Epoch 300/600\n",
      "426/426 [==============================] - 0s 269us/sample - loss: 0.0218 - val_loss: 0.1629\n",
      "Epoch 301/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0219 - val_loss: 0.1527\n",
      "Epoch 302/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0216 - val_loss: 0.1590\n",
      "Epoch 303/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0237 - val_loss: 0.1590\n",
      "Epoch 304/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0228 - val_loss: 0.1602\n",
      "Epoch 305/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0212 - val_loss: 0.1677\n",
      "Epoch 306/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0218 - val_loss: 0.1609\n",
      "Epoch 307/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0210 - val_loss: 0.1550\n",
      "Epoch 308/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0230 - val_loss: 0.1642\n",
      "Epoch 309/600\n",
      "426/426 [==============================] - 0s 231us/sample - loss: 0.0220 - val_loss: 0.1566\n",
      "Epoch 310/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0210 - val_loss: 0.1556\n",
      "Epoch 311/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0205 - val_loss: 0.1679\n",
      "Epoch 312/600\n",
      "426/426 [==============================] - 0s 263us/sample - loss: 0.0206 - val_loss: 0.1568\n",
      "Epoch 313/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.0226 - val_loss: 0.1656\n",
      "Epoch 314/600\n",
      "426/426 [==============================] - 0s 326us/sample - loss: 0.0209 - val_loss: 0.1638\n",
      "Epoch 315/600\n",
      "426/426 [==============================] - 0s 334us/sample - loss: 0.0201 - val_loss: 0.1585\n",
      "Epoch 316/600\n",
      "426/426 [==============================] - 0s 330us/sample - loss: 0.0215 - val_loss: 0.1798\n",
      "Epoch 317/600\n",
      "426/426 [==============================] - 0s 285us/sample - loss: 0.0233 - val_loss: 0.1500\n",
      "Epoch 318/600\n",
      "426/426 [==============================] - 0s 279us/sample - loss: 0.0207 - val_loss: 0.1766\n",
      "Epoch 319/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0238 - val_loss: 0.1568\n",
      "Epoch 320/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0225 - val_loss: 0.1637\n",
      "Epoch 321/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0196 - val_loss: 0.1633\n",
      "Epoch 322/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0226 - val_loss: 0.1861\n",
      "Epoch 323/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0232 - val_loss: 0.1490\n",
      "Epoch 324/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0209 - val_loss: 0.1701\n",
      "Epoch 325/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0192 - val_loss: 0.1601\n",
      "Epoch 326/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0215 - val_loss: 0.1732\n",
      "Epoch 327/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0187 - val_loss: 0.1548\n",
      "Epoch 328/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0192 - val_loss: 0.1725\n",
      "Epoch 329/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0195 - val_loss: 0.1623\n",
      "Epoch 330/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0245 - val_loss: 0.1864\n",
      "Epoch 331/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0215 - val_loss: 0.1578\n",
      "Epoch 332/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0198 - val_loss: 0.1680\n",
      "Epoch 333/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0202 - val_loss: 0.1677\n",
      "Epoch 334/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0190 - val_loss: 0.1646\n",
      "Epoch 335/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0189 - val_loss: 0.1654\n",
      "Epoch 336/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0195 - val_loss: 0.1589\n",
      "Epoch 337/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0202 - val_loss: 0.1758\n",
      "Epoch 338/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0193 - val_loss: 0.1707\n",
      "Epoch 339/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0231 - val_loss: 0.1884\n",
      "Epoch 340/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0226 - val_loss: 0.1677\n",
      "Epoch 341/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0190 - val_loss: 0.1625\n",
      "Epoch 342/600\n",
      "426/426 [==============================] - 0s 289us/sample - loss: 0.0196 - val_loss: 0.1616\n",
      "Epoch 343/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0186 - val_loss: 0.1769\n",
      "Epoch 344/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0184 - val_loss: 0.1689\n",
      "Epoch 345/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0182 - val_loss: 0.1655\n",
      "Epoch 346/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0178 - val_loss: 0.1634\n",
      "Epoch 347/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0177 - val_loss: 0.1614\n",
      "Epoch 348/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0195 - val_loss: 0.1657\n",
      "Epoch 349/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0186 - val_loss: 0.1772\n",
      "Epoch 350/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0201 - val_loss: 0.1548\n",
      "Epoch 351/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0188 - val_loss: 0.1783\n",
      "Epoch 352/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0178 - val_loss: 0.1687\n",
      "Epoch 353/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0173 - val_loss: 0.1740\n",
      "Epoch 354/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0185 - val_loss: 0.1686\n",
      "Epoch 355/600\n",
      "426/426 [==============================] - 0s 219us/sample - loss: 0.0170 - val_loss: 0.1623\n",
      "Epoch 356/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0189 - val_loss: 0.1655\n",
      "Epoch 357/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0175 - val_loss: 0.1948\n",
      "Epoch 358/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0178 - val_loss: 0.1541\n",
      "Epoch 359/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0244 - val_loss: 0.2023\n",
      "Epoch 360/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.0236 - val_loss: 0.1644\n",
      "Epoch 361/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.0177 - val_loss: 0.1705\n",
      "Epoch 362/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0179 - val_loss: 0.1701\n",
      "Epoch 363/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0193 - val_loss: 0.1598\n",
      "Epoch 364/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0181 - val_loss: 0.1778\n",
      "Epoch 365/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0159 - val_loss: 0.1753\n",
      "Epoch 366/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.0185 - val_loss: 0.2014\n",
      "Epoch 367/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0226 - val_loss: 0.1666\n",
      "Epoch 368/600\n",
      "426/426 [==============================] - 0s 316us/sample - loss: 0.0165 - val_loss: 0.1729\n",
      "Epoch 369/600\n",
      "426/426 [==============================] - 0s 322us/sample - loss: 0.0156 - val_loss: 0.1756\n",
      "Epoch 370/600\n",
      "426/426 [==============================] - 0s 296us/sample - loss: 0.0159 - val_loss: 0.1835\n",
      "Epoch 371/600\n",
      "426/426 [==============================] - 0s 310us/sample - loss: 0.0162 - val_loss: 0.1714\n",
      "Epoch 372/600\n",
      "426/426 [==============================] - 0s 308us/sample - loss: 0.0172 - val_loss: 0.1834\n",
      "Epoch 373/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.0172 - val_loss: 0.1739\n",
      "Epoch 374/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0168 - val_loss: 0.1773\n",
      "Epoch 375/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0155 - val_loss: 0.1813\n",
      "Epoch 376/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0161 - val_loss: 0.1685\n",
      "Epoch 377/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0168 - val_loss: 0.2011\n",
      "Epoch 378/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0150 - val_loss: 0.1689\n",
      "Epoch 379/600\n",
      "426/426 [==============================] - 0s 244us/sample - loss: 0.0169 - val_loss: 0.1810\n",
      "Epoch 380/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0168 - val_loss: 0.1794\n",
      "Epoch 381/600\n",
      "426/426 [==============================] - 0s 296us/sample - loss: 0.0183 - val_loss: 0.1942\n",
      "Epoch 382/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.0189 - val_loss: 0.1670\n",
      "Epoch 383/600\n",
      "426/426 [==============================] - 0s 300us/sample - loss: 0.0175 - val_loss: 0.1972\n",
      "Epoch 384/600\n",
      "426/426 [==============================] - 0s 296us/sample - loss: 0.0149 - val_loss: 0.1634\n",
      "Epoch 385/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.0179 - val_loss: 0.2143\n",
      "Epoch 386/600\n",
      "426/426 [==============================] - 0s 298us/sample - loss: 0.0155 - val_loss: 0.1789\n",
      "Epoch 387/600\n",
      "426/426 [==============================] - 0s 279us/sample - loss: 0.0198 - val_loss: 0.1981\n",
      "Epoch 388/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0157 - val_loss: 0.1738\n",
      "Epoch 389/600\n",
      "426/426 [==============================] - 0s 303us/sample - loss: 0.0144 - val_loss: 0.1792\n",
      "Epoch 390/600\n",
      "426/426 [==============================] - 0s 267us/sample - loss: 0.0144 - val_loss: 0.1837\n",
      "Epoch 391/600\n",
      "426/426 [==============================] - 0s 285us/sample - loss: 0.0145 - val_loss: 0.1884\n",
      "Epoch 392/600\n",
      "426/426 [==============================] - 0s 284us/sample - loss: 0.0148 - val_loss: 0.1941\n",
      "Epoch 393/600\n",
      "426/426 [==============================] - 0s 240us/sample - loss: 0.0143 - val_loss: 0.1911\n",
      "Epoch 394/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0136 - val_loss: 0.1780\n",
      "Epoch 395/600\n",
      "426/426 [==============================] - 0s 234us/sample - loss: 0.0135 - val_loss: 0.1873\n",
      "Epoch 396/600\n",
      "426/426 [==============================] - 0s 236us/sample - loss: 0.0153 - val_loss: 0.2008\n",
      "Epoch 397/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0143 - val_loss: 0.1848\n",
      "Epoch 398/600\n",
      "426/426 [==============================] - 0s 252us/sample - loss: 0.0140 - val_loss: 0.1795\n",
      "Epoch 399/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0136 - val_loss: 0.1835\n",
      "Epoch 400/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0133 - val_loss: 0.2003\n",
      "Epoch 401/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0139 - val_loss: 0.1943\n",
      "Epoch 402/600\n",
      "426/426 [==============================] - 0s 210us/sample - loss: 0.0143 - val_loss: 0.1973\n",
      "Epoch 403/600\n",
      "426/426 [==============================] - 0s 226us/sample - loss: 0.0149 - val_loss: 0.1824\n",
      "Epoch 404/600\n",
      "426/426 [==============================] - 0s 199us/sample - loss: 0.0160 - val_loss: 0.2177\n",
      "Epoch 405/600\n",
      "426/426 [==============================] - 0s 212us/sample - loss: 0.0198 - val_loss: 0.1605\n",
      "Epoch 406/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0156 - val_loss: 0.2224\n",
      "Epoch 407/600\n",
      "426/426 [==============================] - 0s 233us/sample - loss: 0.0186 - val_loss: 0.1798\n",
      "Epoch 408/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0130 - val_loss: 0.1979\n",
      "Epoch 409/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0133 - val_loss: 0.1793\n",
      "Epoch 410/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0142 - val_loss: 0.2029\n",
      "Epoch 411/600\n",
      "426/426 [==============================] - 0s 200us/sample - loss: 0.0126 - val_loss: 0.1893\n",
      "Epoch 412/600\n",
      "426/426 [==============================] - 0s 207us/sample - loss: 0.0135 - val_loss: 0.1974\n",
      "Epoch 413/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0188 - val_loss: 0.1767\n",
      "Epoch 414/600\n",
      "426/426 [==============================] - 0s 205us/sample - loss: 0.0112 - val_loss: 0.2238\n",
      "Epoch 415/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0142 - val_loss: 0.1687\n",
      "Epoch 416/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.0138 - val_loss: 0.2175\n",
      "Epoch 417/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0117 - val_loss: 0.1780\n",
      "Epoch 418/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0169 - val_loss: 0.2241\n",
      "Epoch 419/600\n",
      "426/426 [==============================] - 0s 330us/sample - loss: 0.0192 - val_loss: 0.1762\n",
      "Epoch 420/600\n",
      "426/426 [==============================] - 0s 389us/sample - loss: 0.0159 - val_loss: 0.2104\n",
      "Epoch 421/600\n",
      "426/426 [==============================] - 0s 314us/sample - loss: 0.0140 - val_loss: 0.1847\n",
      "Epoch 422/600\n",
      "426/426 [==============================] - 0s 455us/sample - loss: 0.0155 - val_loss: 0.2162\n",
      "Epoch 423/600\n",
      "426/426 [==============================] - 0s 422us/sample - loss: 0.0126 - val_loss: 0.1888\n",
      "Epoch 424/600\n",
      "426/426 [==============================] - 0s 289us/sample - loss: 0.0129 - val_loss: 0.2062\n",
      "Epoch 425/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0132 - val_loss: 0.2021\n",
      "Epoch 426/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 0.0122 - val_loss: 0.1863\n",
      "Epoch 427/600\n",
      "426/426 [==============================] - 0s 277us/sample - loss: 0.0111 - val_loss: 0.1966\n",
      "Epoch 428/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0122 - val_loss: 0.2155\n",
      "Epoch 429/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0173 - val_loss: 0.1867\n",
      "Epoch 430/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0117 - val_loss: 0.2128\n",
      "Epoch 431/600\n",
      "426/426 [==============================] - 0s 274us/sample - loss: 0.0113 - val_loss: 0.1999\n",
      "Epoch 432/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0104 - val_loss: 0.2105\n",
      "Epoch 433/600\n",
      "426/426 [==============================] - 0s 258us/sample - loss: 0.0110 - val_loss: 0.1904\n",
      "Epoch 434/600\n",
      "426/426 [==============================] - 0s 218us/sample - loss: 0.0113 - val_loss: 0.2035\n",
      "Epoch 435/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0148 - val_loss: 0.1982\n",
      "Epoch 436/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0115 - val_loss: 0.1975\n",
      "Epoch 437/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0105 - val_loss: 0.1998\n",
      "Epoch 438/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.0104 - val_loss: 0.1947\n",
      "Epoch 439/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0099 - val_loss: 0.2099\n",
      "Epoch 440/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0112 - val_loss: 0.2044\n",
      "Epoch 441/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0105 - val_loss: 0.2116\n",
      "Epoch 442/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0098 - val_loss: 0.2031\n",
      "Epoch 443/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0096 - val_loss: 0.2003\n",
      "Epoch 444/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0102 - val_loss: 0.2199\n",
      "Epoch 445/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0105 - val_loss: 0.1987\n",
      "Epoch 446/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0102 - val_loss: 0.2285\n",
      "Epoch 447/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0107 - val_loss: 0.1938\n",
      "Epoch 448/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0094 - val_loss: 0.2126\n",
      "Epoch 449/600\n",
      "426/426 [==============================] - 0s 256us/sample - loss: 0.0091 - val_loss: 0.2065\n",
      "Epoch 450/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0089 - val_loss: 0.2113\n",
      "Epoch 451/600\n",
      "426/426 [==============================] - 0s 191us/sample - loss: 0.0134 - val_loss: 0.2707\n",
      "Epoch 452/600\n",
      "426/426 [==============================] - 0s 242us/sample - loss: 0.0113 - val_loss: 0.2008\n",
      "Epoch 453/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0120 - val_loss: 0.2288\n",
      "Epoch 454/600\n",
      "426/426 [==============================] - 0s 314us/sample - loss: 0.0093 - val_loss: 0.2143\n",
      "Epoch 455/600\n",
      "426/426 [==============================] - 0s 245us/sample - loss: 0.0095 - val_loss: 0.2291\n",
      "Epoch 456/600\n",
      "426/426 [==============================] - 0s 265us/sample - loss: 0.0090 - val_loss: 0.2136\n",
      "Epoch 457/600\n",
      "426/426 [==============================] - 0s 232us/sample - loss: 0.0097 - val_loss: 0.2305\n",
      "Epoch 458/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0093 - val_loss: 0.2159\n",
      "Epoch 459/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 215us/sample - loss: 0.0105 - val_loss: 0.2037\n",
      "Epoch 460/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0084 - val_loss: 0.2117\n",
      "Epoch 461/600\n",
      "426/426 [==============================] - 0s 205us/sample - loss: 0.0080 - val_loss: 0.2217\n",
      "Epoch 462/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0079 - val_loss: 0.2137\n",
      "Epoch 463/600\n",
      "426/426 [==============================] - 0s 233us/sample - loss: 0.0083 - val_loss: 0.2315\n",
      "Epoch 464/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.0097 - val_loss: 0.2121\n",
      "Epoch 465/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0092 - val_loss: 0.2265\n",
      "Epoch 466/600\n",
      "426/426 [==============================] - 0s 289us/sample - loss: 0.0080 - val_loss: 0.2015\n",
      "Epoch 467/600\n",
      "426/426 [==============================] - 0s 282us/sample - loss: 0.0092 - val_loss: 0.2206\n",
      "Epoch 468/600\n",
      "426/426 [==============================] - 0s 277us/sample - loss: 0.0081 - val_loss: 0.2137\n",
      "Epoch 469/600\n",
      "426/426 [==============================] - 0s 249us/sample - loss: 0.0079 - val_loss: 0.2242\n",
      "Epoch 470/600\n",
      "426/426 [==============================] - 0s 238us/sample - loss: 0.0080 - val_loss: 0.2052\n",
      "Epoch 471/600\n",
      "426/426 [==============================] - 0s 303us/sample - loss: 0.0102 - val_loss: 0.2416\n",
      "Epoch 472/600\n",
      "426/426 [==============================] - 0s 312us/sample - loss: 0.0083 - val_loss: 0.1995\n",
      "Epoch 473/600\n",
      "426/426 [==============================] - 0s 338us/sample - loss: 0.0085 - val_loss: 0.2472\n",
      "Epoch 474/600\n",
      "426/426 [==============================] - 0s 352us/sample - loss: 0.0108 - val_loss: 0.2013\n",
      "Epoch 475/600\n",
      "426/426 [==============================] - 0s 281us/sample - loss: 0.0075 - val_loss: 0.2310\n",
      "Epoch 476/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0073 - val_loss: 0.2165\n",
      "Epoch 477/600\n",
      "426/426 [==============================] - 0s 165us/sample - loss: 0.0073 - val_loss: 0.2201\n",
      "Epoch 478/600\n",
      "426/426 [==============================] - 0s 198us/sample - loss: 0.0076 - val_loss: 0.2340\n",
      "Epoch 479/600\n",
      "426/426 [==============================] - 0s 148us/sample - loss: 0.0132 - val_loss: 0.1907\n",
      "Epoch 480/600\n",
      "426/426 [==============================] - 0s 215us/sample - loss: 0.0194 - val_loss: 0.2633\n",
      "Epoch 481/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0114 - val_loss: 0.2333\n",
      "Epoch 482/600\n",
      "426/426 [==============================] - 0s 199us/sample - loss: 0.0089 - val_loss: 0.2205\n",
      "Epoch 483/600\n",
      "426/426 [==============================] - 0s 199us/sample - loss: 0.0080 - val_loss: 0.2370\n",
      "Epoch 484/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0074 - val_loss: 0.2209\n",
      "Epoch 485/600\n",
      "426/426 [==============================] - 0s 196us/sample - loss: 0.0071 - val_loss: 0.2186\n",
      "Epoch 486/600\n",
      "426/426 [==============================] - 0s 191us/sample - loss: 0.0070 - val_loss: 0.2256\n",
      "Epoch 487/600\n",
      "426/426 [==============================] - 0s 212us/sample - loss: 0.0066 - val_loss: 0.2177\n",
      "Epoch 488/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.0069 - val_loss: 0.2430\n",
      "Epoch 489/600\n",
      "426/426 [==============================] - 0s 181us/sample - loss: 0.0065 - val_loss: 0.2160\n",
      "Epoch 490/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0125 - val_loss: 0.2798\n",
      "Epoch 491/600\n",
      "426/426 [==============================] - 0s 196us/sample - loss: 0.0132 - val_loss: 0.1953\n",
      "Epoch 492/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0152 - val_loss: 0.2820\n",
      "Epoch 493/600\n",
      "426/426 [==============================] - 0s 195us/sample - loss: 0.0118 - val_loss: 0.1933\n",
      "Epoch 494/600\n",
      "426/426 [==============================] - 0s 201us/sample - loss: 0.0084 - val_loss: 0.2761\n",
      "Epoch 495/600\n",
      "426/426 [==============================] - 0s 176us/sample - loss: 0.0097 - val_loss: 0.2016\n",
      "Epoch 496/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0070 - val_loss: 0.2434\n",
      "Epoch 497/600\n",
      "426/426 [==============================] - 0s 198us/sample - loss: 0.0062 - val_loss: 0.2188\n",
      "Epoch 498/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0065 - val_loss: 0.2352\n",
      "Epoch 499/600\n",
      "426/426 [==============================] - 0s 205us/sample - loss: 0.0065 - val_loss: 0.2308\n",
      "Epoch 500/600\n",
      "426/426 [==============================] - 0s 209us/sample - loss: 0.0059 - val_loss: 0.2352\n",
      "Epoch 501/600\n",
      "426/426 [==============================] - 0s 202us/sample - loss: 0.0061 - val_loss: 0.2297\n",
      "Epoch 502/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0064 - val_loss: 0.2411\n",
      "Epoch 503/600\n",
      "426/426 [==============================] - 0s 196us/sample - loss: 0.0062 - val_loss: 0.2321\n",
      "Epoch 504/600\n",
      "426/426 [==============================] - 0s 166us/sample - loss: 0.0062 - val_loss: 0.2325\n",
      "Epoch 505/600\n",
      "426/426 [==============================] - 0s 214us/sample - loss: 0.0062 - val_loss: 0.2406\n",
      "Epoch 506/600\n",
      "426/426 [==============================] - 0s 210us/sample - loss: 0.0059 - val_loss: 0.2321\n",
      "Epoch 507/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0062 - val_loss: 0.2292\n",
      "Epoch 508/600\n",
      "426/426 [==============================] - 0s 193us/sample - loss: 0.0058 - val_loss: 0.2342\n",
      "Epoch 509/600\n",
      "426/426 [==============================] - 0s 198us/sample - loss: 0.0055 - val_loss: 0.2379\n",
      "Epoch 510/600\n",
      "426/426 [==============================] - 0s 196us/sample - loss: 0.0055 - val_loss: 0.2346\n",
      "Epoch 511/600\n",
      "426/426 [==============================] - 0s 203us/sample - loss: 0.0054 - val_loss: 0.2361\n",
      "Epoch 512/600\n",
      "426/426 [==============================] - 0s 192us/sample - loss: 0.0056 - val_loss: 0.2327\n",
      "Epoch 513/600\n",
      "426/426 [==============================] - 0s 206us/sample - loss: 0.0058 - val_loss: 0.2406\n",
      "Epoch 514/600\n",
      "426/426 [==============================] - 0s 175us/sample - loss: 0.0052 - val_loss: 0.2348\n",
      "Epoch 515/600\n",
      "426/426 [==============================] - 0s 194us/sample - loss: 0.0054 - val_loss: 0.2515\n",
      "Epoch 516/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0056 - val_loss: 0.2236\n",
      "Epoch 517/600\n",
      "426/426 [==============================] - 0s 179us/sample - loss: 0.0055 - val_loss: 0.2666\n",
      "Epoch 518/600\n",
      "426/426 [==============================] - 0s 215us/sample - loss: 0.0053 - val_loss: 0.2237\n",
      "Epoch 519/600\n",
      "426/426 [==============================] - 0s 193us/sample - loss: 0.0064 - val_loss: 0.2497\n",
      "Epoch 520/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0048 - val_loss: 0.2366\n",
      "Epoch 521/600\n",
      "426/426 [==============================] - 0s 194us/sample - loss: 0.0056 - val_loss: 0.2590\n",
      "Epoch 522/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.0056 - val_loss: 0.2282\n",
      "Epoch 523/600\n",
      "426/426 [==============================] - 0s 186us/sample - loss: 0.0048 - val_loss: 0.2536\n",
      "Epoch 524/600\n",
      "426/426 [==============================] - 0s 159us/sample - loss: 0.0052 - val_loss: 0.2345\n",
      "Epoch 525/600\n",
      "426/426 [==============================] - 0s 226us/sample - loss: 0.0068 - val_loss: 0.2794\n",
      "Epoch 526/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.0055 - val_loss: 0.2254\n",
      "Epoch 527/600\n",
      "426/426 [==============================] - 0s 297us/sample - loss: 0.0049 - val_loss: 0.2557\n",
      "Epoch 528/600\n",
      "426/426 [==============================] - 0s 404us/sample - loss: 0.0048 - val_loss: 0.2478\n",
      "Epoch 529/600\n",
      "426/426 [==============================] - 0s 340us/sample - loss: 0.0051 - val_loss: 0.2426\n",
      "Epoch 530/600\n",
      "426/426 [==============================] - 0s 357us/sample - loss: 0.0047 - val_loss: 0.2424\n",
      "Epoch 531/600\n",
      "426/426 [==============================] - 0s 375us/sample - loss: 0.0046 - val_loss: 0.2502\n",
      "Epoch 532/600\n",
      "426/426 [==============================] - 0s 359us/sample - loss: 0.0049 - val_loss: 0.2343\n",
      "Epoch 533/600\n",
      "426/426 [==============================] - 0s 333us/sample - loss: 0.0059 - val_loss: 0.2736\n",
      "Epoch 534/600\n",
      "426/426 [==============================] - 0s 443us/sample - loss: 0.0059 - val_loss: 0.2573\n",
      "Epoch 535/600\n",
      "426/426 [==============================] - 0s 321us/sample - loss: 0.0049 - val_loss: 0.2302\n",
      "Epoch 536/600\n",
      "426/426 [==============================] - 0s 293us/sample - loss: 0.0057 - val_loss: 0.2824\n",
      "Epoch 537/600\n",
      "426/426 [==============================] - 0s 319us/sample - loss: 0.0056 - val_loss: 0.2356\n",
      "Epoch 538/600\n",
      "426/426 [==============================] - 0s 326us/sample - loss: 0.0069 - val_loss: 0.2681\n",
      "Epoch 539/600\n",
      "426/426 [==============================] - 0s 321us/sample - loss: 0.0053 - val_loss: 0.2444\n",
      "Epoch 540/600\n",
      "426/426 [==============================] - 0s 312us/sample - loss: 0.0050 - val_loss: 0.2533\n",
      "Epoch 541/600\n",
      "426/426 [==============================] - 0s 422us/sample - loss: 0.0048 - val_loss: 0.2581\n",
      "Epoch 542/600\n",
      "426/426 [==============================] - 0s 321us/sample - loss: 0.0047 - val_loss: 0.2486\n",
      "Epoch 543/600\n",
      "426/426 [==============================] - 0s 338us/sample - loss: 0.0044 - val_loss: 0.2580\n",
      "Epoch 544/600\n",
      "426/426 [==============================] - 0s 331us/sample - loss: 0.0046 - val_loss: 0.2584\n",
      "Epoch 545/600\n",
      "426/426 [==============================] - 0s 321us/sample - loss: 0.0040 - val_loss: 0.2486\n",
      "Epoch 546/600\n",
      "426/426 [==============================] - 0s 317us/sample - loss: 0.0047 - val_loss: 0.2667\n",
      "Epoch 547/600\n",
      "426/426 [==============================] - 0s 303us/sample - loss: 0.0043 - val_loss: 0.2405\n",
      "Epoch 548/600\n",
      "426/426 [==============================] - 0s 333us/sample - loss: 0.0044 - val_loss: 0.2527\n",
      "Epoch 549/600\n",
      "426/426 [==============================] - 0s 325us/sample - loss: 0.0044 - val_loss: 0.2544\n",
      "Epoch 550/600\n",
      "426/426 [==============================] - 0s 296us/sample - loss: 0.0038 - val_loss: 0.2601\n",
      "Epoch 551/600\n",
      "426/426 [==============================] - 0s 286us/sample - loss: 0.0042 - val_loss: 0.2574\n",
      "Epoch 552/600\n",
      "426/426 [==============================] - 0s 270us/sample - loss: 0.0040 - val_loss: 0.2562\n",
      "Epoch 553/600\n",
      "426/426 [==============================] - 0s 291us/sample - loss: 0.0041 - val_loss: 0.2749\n",
      "Epoch 554/600\n",
      "426/426 [==============================] - 0s 243us/sample - loss: 0.0041 - val_loss: 0.2489\n",
      "Epoch 555/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0043 - val_loss: 0.2825\n",
      "Epoch 556/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0053 - val_loss: 0.2610\n",
      "Epoch 557/600\n",
      "426/426 [==============================] - 0s 450us/sample - loss: 0.0043 - val_loss: 0.2506\n",
      "Epoch 558/600\n",
      "426/426 [==============================] - 0s 272us/sample - loss: 0.0046 - val_loss: 0.2672\n",
      "Epoch 559/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0042 - val_loss: 0.2690\n",
      "Epoch 560/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.0039 - val_loss: 0.2558\n",
      "Epoch 561/600\n",
      "426/426 [==============================] - 0s 270us/sample - loss: 0.0045 - val_loss: 0.2570\n",
      "Epoch 562/600\n",
      "426/426 [==============================] - 0s 207us/sample - loss: 0.0041 - val_loss: 0.2591\n",
      "Epoch 563/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0042 - val_loss: 0.2730\n",
      "Epoch 564/600\n",
      "426/426 [==============================] - 0s 210us/sample - loss: 0.0040 - val_loss: 0.2767\n",
      "Epoch 565/600\n",
      "426/426 [==============================] - 0s 298us/sample - loss: 0.0034 - val_loss: 0.2634\n",
      "Epoch 566/600\n",
      "426/426 [==============================] - 0s 200us/sample - loss: 0.0038 - val_loss: 0.2865\n",
      "Epoch 567/600\n",
      "426/426 [==============================] - 0s 240us/sample - loss: 0.0051 - val_loss: 0.2407\n",
      "Epoch 568/600\n",
      "426/426 [==============================] - 0s 220us/sample - loss: 0.0053 - val_loss: 0.2937\n",
      "Epoch 569/600\n",
      "426/426 [==============================] - 0s 223us/sample - loss: 0.0050 - val_loss: 0.2948\n",
      "Epoch 570/600\n",
      "426/426 [==============================] - 0s 296us/sample - loss: 0.0042 - val_loss: 0.2497\n",
      "Epoch 571/600\n",
      "426/426 [==============================] - 0s 262us/sample - loss: 0.0039 - val_loss: 0.2845\n",
      "Epoch 572/600\n",
      "426/426 [==============================] - 0s 280us/sample - loss: 0.0050 - val_loss: 0.2937\n",
      "Epoch 573/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.0040 - val_loss: 0.2667\n",
      "Epoch 574/600\n",
      "426/426 [==============================] - 0s 382us/sample - loss: 0.0035 - val_loss: 0.2808\n",
      "Epoch 575/600\n",
      "426/426 [==============================] - 0s 297us/sample - loss: 0.0034 - val_loss: 0.2700\n",
      "Epoch 576/600\n",
      "426/426 [==============================] - 0s 329us/sample - loss: 0.0041 - val_loss: 0.2817\n",
      "Epoch 577/600\n",
      "426/426 [==============================] - 0s 308us/sample - loss: 0.0040 - val_loss: 0.2818\n",
      "Epoch 578/600\n",
      "426/426 [==============================] - 0s 313us/sample - loss: 0.0043 - val_loss: 0.2908\n",
      "Epoch 579/600\n",
      "426/426 [==============================] - 0s 312us/sample - loss: 0.0047 - val_loss: 0.2374\n",
      "Epoch 580/600\n",
      "426/426 [==============================] - 0s 196us/sample - loss: 0.0073 - val_loss: 0.2890\n",
      "Epoch 581/600\n",
      "426/426 [==============================] - 0s 204us/sample - loss: 0.0046 - val_loss: 0.3027\n",
      "Epoch 582/600\n",
      "426/426 [==============================] - 0s 263us/sample - loss: 0.0045 - val_loss: 0.2730\n",
      "Epoch 583/600\n",
      "426/426 [==============================] - 0s 226us/sample - loss: 0.0032 - val_loss: 0.2811\n",
      "Epoch 584/600\n",
      "426/426 [==============================] - 0s 221us/sample - loss: 0.0030 - val_loss: 0.2722\n",
      "Epoch 585/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0031 - val_loss: 0.2780\n",
      "Epoch 586/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0030 - val_loss: 0.2784\n",
      "Epoch 587/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.0029 - val_loss: 0.2697\n",
      "Epoch 588/600\n",
      "426/426 [==============================] - 0s 253us/sample - loss: 0.0036 - val_loss: 0.2957\n",
      "Epoch 589/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.0031 - val_loss: 0.2703\n",
      "Epoch 590/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.0034 - val_loss: 0.3095\n",
      "Epoch 591/600\n",
      "426/426 [==============================] - 0s 260us/sample - loss: 0.0032 - val_loss: 0.2695\n",
      "Epoch 592/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0031 - val_loss: 0.2750\n",
      "Epoch 593/600\n",
      "426/426 [==============================] - 0s 206us/sample - loss: 0.0030 - val_loss: 0.2839\n",
      "Epoch 594/600\n",
      "426/426 [==============================] - 0s 228us/sample - loss: 0.0032 - val_loss: 0.2993\n",
      "Epoch 595/600\n",
      "426/426 [==============================] - 0s 188us/sample - loss: 0.0029 - val_loss: 0.2748\n",
      "Epoch 596/600\n",
      "426/426 [==============================] - 0s 246us/sample - loss: 0.0027 - val_loss: 0.2807\n",
      "Epoch 597/600\n",
      "426/426 [==============================] - 0s 230us/sample - loss: 0.0029 - val_loss: 0.2862\n",
      "Epoch 598/600\n",
      "426/426 [==============================] - 0s 216us/sample - loss: 0.0027 - val_loss: 0.2794\n",
      "Epoch 599/600\n",
      "426/426 [==============================] - 0s 239us/sample - loss: 0.0025 - val_loss: 0.2846\n",
      "Epoch 600/600\n",
      "426/426 [==============================] - 0s 251us/sample - loss: 0.0025 - val_loss: 0.2832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x173aa1fe1c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x= X_train, y = y_train, epochs = 600, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_df = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x186450fdec8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU1dnA8d8zk0kmK1sStrDKJoKCRnAFxd1WrNUqaNX6Wq217q1VX6ultrZa21pbebXWutUN6lJREWoVd0UChn3fw5KV7JlklvP+cSZkkkySARImkzzfzyefuffOmTvPDeGZM+eeRYwxKKWUin2OaAeglFKqfWhCV0qpLkITulJKdRGa0JVSqovQhK6UUl1EXLTeOD093QwdOjRab6+UUjFp6dKlRcaYjHDPRS2hDx06lJycnGi9vVJKxSQR2d7Sc9rkopRSXYQmdKWU6iI0oSulVBcRURu6iJwLPAY4gaeNMQ81ef5R4PTgbhKQaYzp2Z6BKqW6Bq/XS15eHh6PJ9qhdGput5usrCxcLlfEr2kzoYuIE5gNnAXkAUtEZJ4xZk19GWPM7SHlbwYmHkjgSqnuIy8vj9TUVIYOHYqIRDucTskYQ3FxMXl5eQwbNizi10XS5DIJ2GSM2WKMqQNeBS5spfxM4JWII1BKdSsej4c+ffpoMm+FiNCnT58D/hYTSUIfCOwM2c8LHgsXxBBgGPDhAUWhlOpWNJm37WB+R5Ek9HBnbWnO3RnAa8YYf9gTiVwvIjkiklNYWBhpjI0s2VbCIwvX4Q/otL9KKRUqkoSeBwwK2c8CdrdQdgatNLcYY54yxmQbY7IzMsIOdGpT7o5SZi/aTGWt76Ber5RSKSkp0Q6hQ0SS0JcAI0VkmIjEY5P2vKaFRGQ00Av4sn1DbCzVbe/jakJXSqnG2kzoxhgfcBOwEFgLzDXGrBaRB0RkekjRmcCrpoOXQEp12y48FR5vR76NUqobMMZw5513Mm7cOMaPH8+cOXMA2LNnD1OmTGHChAmMGzeOTz/9FL/fzw9+8IP9ZR999NEoR99cRP3QjTHzgflNjt3fZH9W+4XVsvoaeoVHa+hKxbpfvb2aNbvL2/WcYwek8csLjoqo7BtvvEFubi7Lly+nqKiI448/nilTpvDyyy9zzjnncO+99+L3+6muriY3N5ddu3axatUqAEpLS9s17vYQcyNFs/I/5AnXo1RW10Q7FKVUjPvss8+YOXMmTqeTvn37MnXqVJYsWcLxxx/Ps88+y6xZs1i5ciWpqakMHz6cLVu2cPPNN7NgwQLS0tKiHX4zUZtt8WClenZxnnMJ8yvLaKH3pFIqRkRak+4oLbUQT5kyhU8++YR3332XK6+8kjvvvJOrrrqK5cuXs3DhQmbPns3cuXN55plnDnPErYu5GnpCsp1RwFPZ+b7uKKViy5QpU5gzZw5+v5/CwkI++eQTJk2axPbt28nMzOS6667j2muvZdmyZRQVFREIBLj44ov59a9/zbJly6IdfjMxV0NPSOkFgLe6LMqRKKVi3UUXXcSXX37JMcccg4jw+9//nn79+vH888/zyCOP4HK5SElJ4YUXXmDXrl1cc801BAIBAH73u99FOfrmYi6hxyf1AMBXrTV0pdTBqaysBOxozEceeYRHHnmk0fNXX301V199dbPXdcZaeaiYa3IRt03o/pr2vTOulFKxLuYSOgmp9tGjCV0ppULFXkJ3B7sK1VZENw6llOpkYi+hB2vojjqtoSulVKjYS+iuJPw4cHorox2JUkp1KrGX0EWodSQT79OErpRSoWIvoQO1cSnE+6uiHYZSSnUqMZnQvXHJuP1VLQ7bVUqp9tLa3Onbtm1j3LhxhzGa1sVkQve7Ukmhhqq6sAsjKaVUtxRzI0UB/PGppMo+qmt9pCTE5CUopQDeuxv2rmzfc/YbD+c91OLTd911F0OGDOHGG28EYNasWYgIn3zyCfv27cPr9fKb3/yGCy+88IDe1uPx8OMf/5icnBzi4uL405/+xOmnn87q1au55pprqKurIxAI8PrrrzNgwAAuvfRS8vLy8Pv93HfffVx22WWHdNkQowndJGgNXSl1cGbMmMFtt922P6HPnTuXBQsWcPvtt5OWlkZRUREnnHAC06dPP6CFmmfPng3AypUrWbduHWeffTYbNmzgySef5NZbb+WKK66grq4Ov9/P/PnzGTBgAO+++y4AZWXtMzdVbCb0+FRSpZq9dbrIhVIxrZWadEeZOHEiBQUF7N69m8LCQnr16kX//v25/fbb+eSTT3A4HOzatYv8/Hz69esX8Xk/++wzbr75ZgDGjBnDkCFD2LBhAyeeeCIPPvggeXl5fPe732XkyJGMHz+en/3sZ9x11118+9vf5tRTT22Xa4vJNnRxp5FCDdVaQ1dKHYRLLrmE1157jTlz5jBjxgxeeuklCgsLWbp0Kbm5ufTt2xePx3NA52ypk8bll1/OvHnzSExM5JxzzuHDDz9k1KhRLF26lPHjx3PPPffwwAMPtMdlxWYN3eHuQYL4qKmuAnpHOxylVIyZMWMG1113HUVFRXz88cfMnTuXzMxMXC4XixYtYvv27Qd8zilTpvDSSy8xbdo0NmzYwI4dOxg9ejRbtmxh+PDh3HLLLWzZsoUVK1YwZswYevfuzfe//31SUlJ47rnn2uW6YjKhxyXa4f+11Tqfi1LqwB111FFUVFQwcOBA+vfvzxVXXMEFF1xAdnY2EyZMYMyYMQd8zhtvvJEbbriB8ePHExcXx3PPPUdCQgJz5szhxRdfxOVy0a9fP+6//36WLFnCnXfeicPhwOVy8cQTT7TLdUkkfblF5FzgMcAJPG2MadbwJSKXArMAAyw3xlze2jmzs7NNTk7OwcRMyaf/oPcHdzD/jPc5/9RJB3UOpVR0rF27liOPPDLaYcSEcL8rEVlqjMkOV77NGrqIOIHZwFlAHrBEROYZY9aElBkJ3AOcbIzZJyKZh3ANbYpLTAbA69Hh/0opVS+SJpdJwCZjzBYAEXkVuBBYE1LmOmC2MWYfgDGmoL0DDZUQbHLx1ujwf6VUx1u5ciVXXnllo2MJCQksXrw4ShGFF0lCHwjsDNnPAyY3KTMKQEQ+xzbLzDLGLGh6IhG5HrgeYPDgwQcTLwDxbltD99dqDV2pWGSMOaA+3tE2fvx4cnNzD+t7HszUJpF0Wwz3W2/6TnHASOA0YCbwtIj0bPYiY54yxmQbY7IzMjIONNaGgOKDCb2u+qDPoZSKDrfbTXFxsc7F1ApjDMXFxbjd7gN6XSQ19DxgUMh+FrA7TJmvjDFeYKuIrMcm+CUHFE2kXIkAGK2hKxVzsrKyyMvLo7CwMNqhdGput5usrKwDek0kCX0JMFJEhgG7gBlA0x4s/8bWzJ8TkXRsE8yWA4rkQMQnARCoq+mwt1BKdQyXy8WwYcOiHUaX1GaTizHGB9wELATWAnONMatF5AERmR4sthAoFpE1wCLgTmNMcUcFjcs2uYhXb4oqpVS9iAYWGWPmA/ObHLs/ZNsAdwR/Ol6wyQWv1tCVUqpeTM7lgss2uTh8mtCVUqpebCZ0ZxxeXJrQlVIqRGwmdMDrcBPn14SulFL1YjehOxM1oSulVIiYTeg+pxtX4MDmK1ZKqa4sZhO635lIgvHg8weiHYpSSnUKMZvQA3GJJFKn64oqpVRQzCZ040oiSWrxeDWhK6UUxHJCj0vETS01WkNXSikghhM68UkkUasLRSulVFAMJ/RkEqWOGm1yUUopIIYTusOVRCLahq6UUvViN6EnJJNILTW1vmiHopRSnULMJnRnQhJxEqC2VgcXKaUUxHJCd6cAUOfRVYuUUgpiOKG7ggtF+zy6yIVSSkFMJ3RbQ/fpuqJKKQXEcEKPC9bQ/Z7qKEeilFKdQ8wmdMf+haK1hq6UUhDDCb1+oehArdbQlVIKIkzoInKuiKwXkU0icneY538gIoUikhv8+WH7h9rE/oWiNaErpRRAXFsFRMQJzAbOAvKAJSIyzxizpknROcaYmzogxvDibQ1dE7pSSlmR1NAnAZuMMVuMMXXAq8CFHRtWBFy2DV00oSulFBBZQh8I7AzZzwsea+piEVkhIq+JyKBwJxKR60UkR0RyCgsLDyLcEMEmF/HquqJKKQWRJXQJc8w02X8bGGqMORr4L/B8uBMZY54yxmQbY7IzMjIOLNKmgk0uDp8mdKWUgsgSeh4QWuPOAnaHFjDGFBtjaoO7fweOa5/wWuF04cdJnF8TulJKQWQJfQkwUkSGiUg8MAOYF1pARPqH7E4H1rZfiC2rc7g1oSulVFCbvVyMMT4RuQlYCDiBZ4wxq0XkASDHGDMPuEVEpgM+oAT4QQfGvJ/X4cbp19kWlVIKIkjoAMaY+cD8JsfuD9m+B7infUNrm8/pxuXVhK6UUhDLI0UBvzMR1/6me6WU6t5iPKG7cZta6nyBaIeilFJRF9MJPRCXiFsXilZKKSDGE7qJS8RNnS4UrZRSxHpCdyXahaLrNKErpVRMJ3RxuUmkjmpN6EopFdsJHVcSbqnVNnSllCLGE7ojPolEbUNXSikgxhO6Mz7ZtqHX+qIdilJKRV1sJ/SEJJxi8NTqaFGllIr5hA7g9ehC0UopFdMJ3eVOAcDr0VWLlFIqxhO6XeTCV1sV5UiUUir6YjqhxyXYZeh8tVpDV0qpmE7ojuAydIE6TehKKRXTCb1+oeiA1tCVUqprJHTj1YSulFIxntBtt0U0oSulVKwndFtDp04XilZKqRhP6MEauk8TulJKRZTQReRcEVkvIptE5O5Wyl0iIkZEstsvxFYEa+gOnw79V0qpNhO6iDiB2cB5wFhgpoiMDVMuFbgFWNzeQbYomNCdfq2hK6VUJDX0ScAmY8wWY0wd8CpwYZhyvwZ+Dxy+6rLDiU9cWkNXSikiS+gDgZ0h+3nBY/uJyERgkDHmndZOJCLXi0iOiOQUFhYecLDheB0JxAW0hq6UUpEkdAlzzOx/UsQBPAr8tK0TGWOeMsZkG2OyMzIyIo+yFV6Hmzi/1tCVUiqShJ4HDArZzwJ2h+ynAuOAj0RkG3ACMO9w3Rj1O924ArWH462UUqpTiyShLwFGisgwEYkHZgDz6p80xpQZY9KNMUONMUOBr4DpxpicDom4Cb8zETe11PkCh+PtlFKq02ozoRtjfMBNwEJgLTDXGLNaRB4QkekdHWBb/E43bup0oWilVLcXF0khY8x8YH6TY/e3UPa0Qw8rcsaViFvK8Xj99Eh0Hc63VkqpTiW2R4oCxCXahaLrtIaulOreYj6hG1ciidRRrQldKdXNxXxCF1ciiVKrbehKqW6vCyT0JNzU4dGErpTq5mI+oTsSkrUNXSml6AIJ3ZlQ34bui3YoSikVVbGf0OOTcYihzqPD/5VS3VvMJ/Q4dzIA3tqqKEeilFLR1WUSus9TGeVIlFIqumI+obvcqQD4PRVRjkQppaIr5hO6I5jQTa0mdKVU9xbzCZ34FPtYq00uSqnurQskdNuGHqjThK6U6t5iP6En2CYXtA1dKdXNxX5CDza5iNbQlVLdXOwn9ASb0B1e7YeulOreYj+hu5II4NCErpTq9mI/oYtQ50gkzqcJXSnVvcV+QgfqnEnE+zWhK6W6ty6R0P1xSbhNDXW+QLRDUUqpqIkooYvIuSKyXkQ2icjdYZ6/QURWikiuiHwmImPbP9SW+VzJpOChslan0FVKdV9tJnQRcQKzgfOAscDMMAn7ZWPMeGPMBOD3wJ/aPdJWGFcKyVJDpUcTulKq+4qkhj4J2GSM2WKMqQNeBS4MLWCMKQ/ZTQZM+4XYNhOfQgoeKmq9h/NtlVKqU4mLoMxAYGfIfh4wuWkhEfkJcAcQD0wLdyIRuR64HmDw4MEHGmuLHAmpJOFhj9bQlVLdWCQ1dAlzrFkN3Bgz2xhzBHAX8ItwJzLGPGWMyTbGZGdkZBxYpK1wuFNJFo82uSilurVIEnoeMChkPwvY3Ur5V4HvHEpQB8qZmEoKNdrkopTq1iJJ6EuAkSIyTETigRnAvNACIjIyZPdbwMb2C7FtrsRU3OKlqlrXFVVKdV9ttqEbY3wichOwEHACzxhjVovIA0COMWYecJOInAl4gX3A1R0ZdFPxSWkAeKrK2yiplFJdVyQ3RTHGzAfmNzl2f8j2re0c1wGJS7QJva5GE7pSqvvqEiNFxd0DgEB1WZQjUUqp6OkSCZ1gQvdVl0Y5EKWUip4uldD91fuiHIhSSkVPl0rogRqtoSuluq8uktB72keP3hRVSnVfXSSh214ucXXlBAKHdRoZpZTqNLpGQne68DrcJFNNWY2OFlVKdU9dI6EDvvg0elBFcVVttENRSnVG27+Ab16KdhQdqssk9IC7N72lnKLKumiHopTqjJ49D9668eBe++GDsPbt9o2nA3SZhE5KBhlSRkmVJnSlVDv75Pcw5/vRjqJNXSahO1P7ki5lFFdqk4tSqnvqMgk9vkc/0imjqEITulKqe+oyCd2RmolbvFRW6GhRpVQ7WPoczOoBdVWRv2b1m7DiXx0WUlu6TEInORMAT+neKAeilOoUNn9oE2xTX/89std/8kf7WBGSU0wb41z+9QN444fhn/N74bX/gcL1kb3/Qeg6CT3FLmlXqwldKQXwz4tsgm1q/s8iPEEweftDOlp4qxu2/zY1/PlbsjsXVr0Ob94Q+WsOUNdJ6MEaeqCiIMqBKKW6FE/ItNxVRQ3be3LDfwMA+M99zY9JMN0GOm7t466T0FNsQk/yllBZq4tFK6UOQv5qWPRb27RS37wSmtCri5u/ZvuXzY998Rf76PfB+7+ET//UkMj3roANC9s37qCuk9CT0jEIGVLGjuLqtssrpbqv0Lbw5a/Cs+fb7WfPh48fDjat1Cf0kEn/qkuan+vZc1t+n/xV8Pmf4YNfQcnm1s/TDrpOQnfG4Xf3Ip0yNhZURDsapVRHWTcfdn/TsL/za9sbpWAt5Dxja8WhFv+t+Tk2f2BvUOavhjd/BNs/t0neW2Of/+2AhrZzT8i03PU19D+Pb3y+la+Br8mgxqqixnG+dVPDdvCeX3uLaE3RWOFI60+/6n3kFlRGOxSlVEd5daZ9nBVsClkZ7Cb46hW2Fuz3wuQfNZR/7+dw/HWNz/HixfZx19KGY1/8tXGZ+qaW2tAaejChl+5oXPb1a+HGxY2PPXJEw3b2tZDzj4b95I5J6BHV0EXkXBFZLyKbROTuMM/fISJrRGSFiHwgIkPaP9S2OXoNYVhcMWv3aA1dqW6jvvmkJjgGJfTGZT1/CwMO921r2H7/vsY9Wuq3v/y/hmO7cmDhveHP9X+Twx+Pc8NJNzU+FuzE0d7arKGLiBOYDZwF5AFLRGSeMWZNSLFvgGxjTLWI/Bj4PXBZRwTcqp6DGcBH5O4owRiDiBz2EJRSHaRiL3z25+bHTcA+1gTbpfOWNO8vXrozwjcJ08+8OuQDYtXrEZ4nRGIv6DGo8bHk9AM/TwQiqaFPAjYZY7YYY+qAV4ELQwsYYxYZY+rvRH4FZLVvmBHqORh3oBpv1T7y9tVEJQSlVCs+/RO8dGnk5b01tv82wLp3YfETzcvUJ/R6WxbBo0c1Pjb7+AOLM5z+Ew7udUl9wOlqfKzpfjuJJKEPBEI/3vKCx1pyLfBeuCdE5HoRyRGRnMLCwsijjFTPwQBkSSG7SzWhK9XpfPAr2Bhhl72AH169HJ6aChX5DTXwppomdIDyXQcfY0vOuL/l54LrGjcy+cf2sdfQxseveK3dQmoqkpui4dotwo5/FZHvA9nA1HDPG2OeAp4CyM7Obv+14kISus6LrlSMqim1PVY+f8wO3wd7M7LpIvDPXwBJ6ZCQcnjicve0g4NCP0B6H2FvxJ58KzgToN84eOFCuOwlGHEG1FXC6f/b+Dwjz+qwECNJ6HlAaANQFrC7aSERORO4F5hqjInOlIeNErrOuqhUp7Bvu/2/GXpPy1cLcQnNy/p98HCYPhVbP2646bn/2CftG2db4uIhtX9D7f+oi2DQZFhwN/QZCWOn2+N3bm5oI7/w8YbXjzoPNoRtvGg3kTS5LAFGisgwEYkHZgDzQguIyETgb8B0Y0z0xt67e2IS0hgkRZrQlWrK74V37oCyDmiO8LXw/213Ljx2tO0fHqo2pCda0caGGQrXziOsBXfDjjAjMg9V+qjIyzrjIaWv3R7zbbj4GZj0I/j+63DkBQ3lWrrhOfMV+GVp+OfaSZsJ3RjjA24CFgJrgbnGmNUi8oCIBD+SeARIAf4lIrki0sK/SgcTQXoPY6xrtyZ0pZpa967tC/3vH0dWfunzsHcVBIJNDH8YFb7L3voF8JtM2LO8oWy9+v7aGxY0Pl64zo7ALNkKj2fbGQoDgcb9wpsq2RJZ3E31HQ+jvxX+uYuebPl1478Hp9zRsO+Mh3Mfst82LpwNDof9GXFm428fLRGJrNwhiGhgkTFmPjC/ybH7Q7bPbOe4Dt6gyRy953me2KeDi5TaL381/Otqu12xp+3yvjp4+xa7PeEKmP44VObDl4/DOQ82Llu/1ubfptgmiPMfsQNn0gY0zCVesRfevq3hNc+FSbCeUqgswN62i/AW26UvwNyrmh8/69e2XzlA5hj47t/tItHPnd+4nLtnw7YzAa7/yA777znY9k5xOGHj+5C/0radD54Mt62MLLYo6DpD/+sNPgE3tXj3rIp2JEp1HntDklBVBD3MQpN+7kuNX1OR37hs6OyBOxfbxP63qTaJfzU7+P4rYOmzrb/nwnth5Vzof4zdlwjSU5y7+bGr3oIjpjXs17ff9x3buNzob0F8csO+OGyZrGw72Z/DaY9f/ipMvQt6D287nijregk90/Y/Ta/eSkGFJ8rBKHUYbP+iIWF7a+Cjh5q3aftC/i/U31z01cKTp8KWj+2+MfDvn9jzNe3299mjDduPZ9sRlqU74Z3bYcWrzWOqKoBHxzX+IGkqtN0ZYPnLwTgCMPPV5kPpU/s3fk3GkTDkJPj51oZj33kChp9me5v8shQu/gdMDQ5uT+wFN3xut4eeCjNftsf2n79f+Dh7ZNmeKjEwULFLzeUCQO/hBCSOkY48Vu8uJ3N0mE9wpbqSZ8+zj7PK4IvH4aPf2aaEE0IWUvA2GZfx5Ww44gxbc35hOhwzE879HeS+aH+m3tW4fOiAntpyeOyYtuMKeBu2p94Nm/5rh86DrYUfPaOhuSZUxR4YfV7j0Z43Lm6Y/2TARDj5dtt+Xe/8P9ja+oTLG46JwPhLGp+771Ew7T6YeKXdj0uwv7elz8MRp7d9TZ1c10vocfGYPiM4Kn87q3eXc/rojpkzQalOx5iGyaNCkyk0n8d74f/ahFpv+Su2Jlrv44cPLZaUflC5F7KOtzXr0+8Bd1pDQp/xSvNvAc54O3/K6OAHlAic+7A9R+aYhnKn/rT5+026rvmxcERgSpgVi467OrLXd3JdL6EDzuFTOaHoWd7YWQCMiHY4KhYEAjD/p5D9P9BvfNvlo2nt27ZduH+TWvKTp0Kf4Ax/u3NtgheBr56ATx5pfp6mTSXhyoRKSGs88yDA9L/a9upHj7I16B9+YI/PvxMqMuCH/20oO/A4+zj1LugxsKEZKKUvHHs1jPuubQJJ7N3wmhM6brm2rqjrtaEDjDgDN3V4tn+NaWtRV6UAKnbbvtJPngKr/916WU95y/2uW1Kyxc7ZvfG/bZdtTdkumPN9e+PRWwNFmxqey18Ja4Kxr3rN3oQs32P7cDc15tstv8cVr8H9+2wvj3o/3wqZwZuKYy+E835vtwdMtDX7e3bBrSug1xD7c9GT8P0my7MNPgF+8G5DDbvnYNvefcmzMO1eyDzStmPHxR/Ib0SF6JoJfcBEAAZ5NrC5ULsvqgiEJug3f9RyOYCHBsEL3zmw8+/4yj427elRU2oXR1j2gu09EvDb44EAPHVaw+x+eTnw1OnwaEhPjQf7wePHtfye79wO/wwT5/TH7c3DlvQ72rZPJ6Ta/Rs+g6TecNrdNslPfxwmXW+779V/m0lIgfikhnMk9Q6/iMPQUxpGiDpdtkfK0JNbjkUdkC7Z5EJKJr7k/hxTvoUvNxczIjM12hGpzi503UhfK72j6r/x7fii5TI1pfDBA3DmLxsmbaqfW3vdO/DVk/DfX8L3nofXfwh1IaMmj55hk+2v08H47ao6O7+Gxa0MgGlN4brmx469svH+L0vtt5N3g4No6mvmV7xuF0HuO87uH3E6/DxkcE9wqg3VeXTNGjrgHHk605zLydm0N9qhqAPh98Hjx8Oadhxs7CmDOVdCebMpiBo0bRuut+wF+E1fO2wegmtNBm35yD5u/9LWoL3BD4Kv/s+OyPz6Kbv82Zp5jYe+L7jLfmi8clnjZA62XfuBXjaZ14skmY84E6bcCafc3nDstpVwzm/hZxvh7Aebv+bqt+GsB2w7+/HXwi25tpufM1jPSx8BU++Mie56yuqaNXRAxl1MSu7LOLd+gDGTdbGLWLH6DSjaYNdfrJ/s6FCtmGvnCCnaAD/+wjZrNG2n9YRJ6MbAuz+zq92893M4c5ZtY6/3woVw796GRYJ7HwE/WdwwfP3rp21Pj0P1nSftwJ7BJ9hmlPwwg+ay/wfGBEdf1vcZ7zkYTvyJ3T7pJttTpCyv4TXDptifer2H2R8Vs7psQmfYVDzxvTit5lNW7y5n3MAw8xWr6MtfbecAGX2+rQW/Eex+ZgJ2IYSeg+D0e22b7IHKecYm5fqFegvXwR9G2i58lzxje1TsXQUn39K8hj73Klvrrl+6LOeZ5hNMgW3HrleyGf44uqGLYGvJfMy37QfM6PPtqvD17t5h18bc9qlty967Aoad2tClcOL37U3O9FFw+Vw7v7jXY28utmVE55mhQ3UMiVYvkOzsbJOTk9Oh71H75q34c1/hgSPf4qEZJ3boe8Wc4s1QXQKDWljJpWYfrH3HJqdTbgtfpi111XaBgj4jbFPAi9+Fab+AwSfauaxPvg3evN6WHfsdOOkWeHpa8/MMmgzXLGg8kCSU32unUh1xht2v/5v+Vc/w5TvK5B/b3iUDjoXjfgBLn7P9o4efBl//HRbe01D251vth1Rlgf2QiU+x3f8u+6e9nuoS2+6++QN7rnrG2Dm2E1q5L0wSdX4AABlnSURBVFQ/m2KP1tahUbFKRJYaY7LDPteVEzrbPofnzud/Hbfx4H2ztNmlXsAPDwRrvLPKmj+/d2XjpoU71kFaf5ugizbAgAmQ+wpkjGroW1xXBY4424Mhb6mdaMnnsQm9qXN+awe2NBVuvuhBk+38IINOsH2Sdy2zXdtKd9qmh5Fn23btj35nyx89w7ZDT/i+HfF4KHoOgdLtkZWd8XJDk0dLKvLtUmiessa/933bIC2roe1aqVZ034QeCFD9+9F8U5VB35sWMqJvN+7tUlVk52kuXA+zJzUc/+kGSO1rmz22fQbH/xAeGty8p4cryTYB7PzKbtffHLz4H7a921cD4y62fZU//HX7xf2/u4Pzhcxpn/ONPMcugZY+GorWhy8Tn2JHExasbf19L3gM3r7Vbt+4uPFoxpbUVdkukgfThKQU3TmhA2WL/kKPj+/jv8N+zplXh5nLuSsr2QqvXQMn3Wy7v8142Ta11E8rWm/EmbZrXEs9PQ7VkJNhe3BSpN7D7SCb+sQaqr42Xm/mHBgdvOH49d9trX7oqbbJISndDiOvbDLzX72s423b+79vtG3kg0+A2kobC9i28Xd/aps5Xr8W0gY2DEX/Zant2eEpsx+ANaW2+WPNW/Cd/7OzCO5ZDkd+2zaBVOa3PLGTUu2sWyd0An5WPHIuo2pycV73X1wDI5hUqDPx+2yira/RVZfYGmRcvG3nXveuna9axD5XthPWv2fnz9iyyPYjDtVnBBRvav4+TR17NSx73m6P+bbtPw2Q0ANqwzTTTPsFfPgbu330DDsr3pwrbJPMdR/CP86xie/WXDvIZmC2vWlYsLah+985D8I3L8F/7rWjCU+6ue04PeV2oA/YubEHHgcOl/3WEQm/z95knHS9vV+w9WM7gEapTqp7J3Tg86XfMHLed8iUUrh5WcN8F51JwG8Hmxx3TeP43r7Nji68N9+2T/+qp63JTrreNgds/I+tdV78D/hTBF/56zni7BDvRQ9Cxmj7IVGZbz8Axl1sz7f5QztfSHK6ra0+NNiu4jL4RDsIZcgpMPl6+yHR9yjYsdhOCjU02P7u9wJi24br/8464j7GrB52xOINn7X/uZXqZLp9QjfG8MRff8eNJQ9jnPHILbl2MntvdXSSe121TXzukK6Uu5bC34M9PC59wdY8d3xpFxcAiEu0X+v3bW1+vpYkpUN1kd0++VboP8FOhDRosn1/V2Lj8r5a28bbUvtu2S7b1S8+qWHip87AU2Zn6mt6PUp1Qa0l9G5xW11ESM2ewYfvLWQauY3nwwjXy6M91FU1rIay4T+AgVHn2P0nT7F9lr/9ZzjqO3bpq82LGl4bbkktX03jZO5w2b7JTRP8mbPgpFsb1i/Medb26T7+2sblwvWoiEsIvxJ7vdBucJ0lmUPjD0alurFuUUMHyNtXzfRH3uGLxNtw+6sanrhjrW1mcPeAoy5qvCRVSza+D+kjodfQhmPeGpuYC9fBpvfh/fttk0RtuR0cApAxpqFtuy3JmXbVl4wj7Q299JG2X/iOL+CevIZ+yMv+aW80Zh7Z0LaulOqyDrnJRUTOBR4DnMDTxpiHmjw/BfgzcDQwwxjzWlvnPNwJHeCWV77ho/X5LJv8MXGL/695gbEXwiXPNR7AUrIFxGkTfvFm29uhfqj3pOth+Ol2OPV7d9q+0ju/aj2IpjcVM8bYeab9tbZXRn0tHmyzS3xKQzzG2Buh2uVNqW7rkBK6iDiBDcBZQB6wBJhpjFkTUmYokAb8DJjXWRP655uKuOLpxfzPycO4f3yJ7UMcrsdHfKpNmmOnwxd/Pbg3O/1e2+MEsR8KnlLIvtb2GCndDv/8Llz2QmRDtpVSKuhQ29AnAZuMMVuCJ3sVuBDYn9CNMduCzwUOOdoOdNIRffjusQN58avt3DTtDHr/+At7Y/SrJ+wqMH1G2EmcwCbdSJJ55lE2+U+4ws6yF58M33uu9X7JKRlwz452uSallKoXSUIfCOwM2c8DJh/Mm4nI9cD1AIMHH/65lEWEG6Yewb+/2cV9b63i8ZkTkbgEu6L36cGh6PXfWIo320VtM8fYpbX2bbNNIH2OgJRMOyzcnrThDcZfYrsDdqYbhkqpbiOShB4uOx3UnVRjzFPAU2CbXA7mHIdqVN9U7jxnDA8vWMfEQT354anDGxeoT8bpI+xPvb5HtX1yp6v9AlVKqQMUyQIXecCgkP0soJWVAjq/G6YO58wj+/Kbd9dyx5xc8stbWaFGKaViRCQJfQkwUkSGiUg8MANox+VkDj8R4RffOpKkeCdvfLOLF7+KcEY9pZTqxNpM6MYYH3ATsBBYC8w1xqwWkQdEZDqAiBwvInnA94C/icjqjgy6PQxNT+bre89kbP803vxmF2U13miHpJRSh6TbDCxqyRebirj8aTvD38s/nMxJI9KjHJFSSrWstW6LXXaR6EidNCKd284cCcD1/1zKF5uKohyRUkodnG6f0AFuO3MUX94zjYzUBC5/ejHf+sun/Hb+WnaV1jQr+5OXlvH28pi+J6yU6qI0oQf175HI/FtO5a5zx7B6dzlPfbKFC/76GTuKq/eXKaqs5d2Ve7j5lW+iGKlSSoWnCT1EYryTH592BP+9YwrHZPWgpKqOc/78Cfe/tYpVu8pYs7thRZ91e8sp9+iNVKVU59Htb4q2ZntxFX/9cBPzcndT528+q8GEQT35909OjkJkSqnuqtsvcHGoCitqeWfFbj5cV8DpozN54J3909hw1YlDGNMvje8eOxC3y3lA591b5qGsxsvoft148Wql1AHRhN7O/rsmn40FlazcVcr8lXsBiHc6SE+JJyPNTY9EF8dk9eC00ZkEjOFH/1zK01dnc+zgXo3OM+2PH7GlsIoVs84mza3TBiil2qYJvYMYY1i9u5ztxdWs2FXK11tLqPMF8PkN6/MrmpU/96h+9OvhJj7OwYjMFH7+ml344pcXjOWak4cBsDKvjFH9UkiIi7y2/8Pnl3DskF7ceNqItgtHaPnOUo7ITCElofMvapW7s5QeiS6GpUewOIlSMU4TehTsKq3hneW7Kamuo0eiizeW7WJTQWXYsikJcWT1SmRPsAnmxOF9uOrEIQzLSKas2sui9YUkxTs5bXQGDy9Yx4UTBnJptp1e59rnlvDBugIAtj30LarrfCTFN07Cf/rPemq8fu791thm7x1OaXUdEx54n3OP6seTVx53CL+Fw2Po3e8C9vqV6uq6/Zqi0TCwZyI/mtqwAPWNp43A5w+Qt6+GnfuqWbBqL5W1Pk4fncmi9QXU1PlxOR2s3FXGl1uK+XJLcbNz/un9DQAs2bqP5TtLWZ5XyqpdDT1vZs1bzXNfbOMvMyeSt6+a44f25uisHvzlQ7uIx81njCTR5cTrD+xP+sYY6vyBRt8IVu6yKyot3lpMnS9AQYWHrF5J7f9Lagc1df5oh6BUp6E19E6ooMLD3jIPW4uqSIhzUFXrx+Pzs2hdIVeeOITH/ruBlbvK8AUMcQ7htxeN51dvr6Gy1tfqecf2TyNvXzXlHh8nj+iDQ4RPNxYxsGcif718Isu27yMpPo6NBRU8+/m2Rq9d9LPTGNonCQmZ690YQ2FFLZlpbvwBg9Nx+OeBX7O7nPP/8ikAy395Nj0S9V6E6tq0yaULCgQMfmPYV11HZqqbnSXVLN2+j55JLvaWeZg4uBfL80p5e/lu+vdwMyw9hXnLd7N2T3nbJ2/BkD5JnDCsD4u3FtMj0cWaPeV4/YasXomUVnuZPKw3Q/ok89OzR+FyOiio8LAxv5L0lARG9UvBIYLL2Xzow8b8CoZnpOz/QHhtaR4nj+hD/x6JeP0B4hzS6IMk1Fu5u7j11VwA/nblcZxzVCsrRUXgi81FjBvYQ29Sq05LE7pqpKzGi8tpk+uG/ArinQ42FlQyfmAP1u+t4LNNRVyaPYi/friRvmluJgzqybsr91BQUcuWgkoq2vgm0BKnQxg3II0Jg3ryxeZixg3swdj+aTw4fy1TR2Xwm++M4w//Wc9bubsZlp7MbWeO5NfvrGFIn2SumDyYE4b3YWdJNYN6J7Eir5Szx/bjgXfW8PLiHSS4HEwbk8ljMyZijGFrURXD0pP3fxAYY1r8UKg3N2cnP39tBRdOGMBjMya2WvbJjzczqm8K08b0PajfRUfaWWJHNw/q3TmbydSh0YSu2lVVrY84p7CzpJoRmankl3vYVlRFVZ0Pp8PBNzv2UecLkJwQx6RhvdlUUMknGwrp3yORr7YUs2ZPOUf2T2NrUSUeb9vL0Io0rAwYzklH9GFw7yTeXr6bP146gSXbSvjHZ1v50dThnD46k/SUBP6wcD3L80p5+OKjOXVk+v7kXuvz8/sF6zk6qwe3zcnFGEhNiGPJL85k+c5SXluax4MXjSc+ruGbRf1NY2j9Rmx1nQ9jIDmkp9C2oiqGNGm6ak8er58x9y2gZ5KL3PvP7pD3UNGlCV11KpW1PlIS4sgv97C5sJJjB/di9e4yVu0qp8brZ1CvJDYWVHDmkX0ZnpGM12+Yu2Qnf/tkM0WVdfvPM7Z/GhvyK/j71dn0Sorn0ie/DDuit6neyfH0S3NTXedjW8hcPQCzLhjLrLfXMGlob77eVgJAn+R4Kjw++vVwc8XkwcxZspMtRVUAvHnjSWwrriI9JYGTj0jHEWw2Kqyo5drnl7C71MPi/z0Dp0P4YG0+1z6fw28vGs/lkxvW1DXG8J81+eworuassX0ZGqb7ZWWtj2Xb9zFlVEar17Zg1R5ueHEZ0D69fj7eUMjsDzfx4g8nN/pQ604qPF4+31TEueP6RzsUQBO66iLq/1ZDa7dVtb79NeCCCg852+x9hKOzevL5piISXU5W7ioj0eXk4uOymL9yDyvySskvr2VjQQVVtX5OHN6H+DgHWb0S+enZo3nxq+3MXrSJ/HIPYwekkZnqprLWx9dbS/a/76DeiewsaTwbp4jtglpV6yPQ5L9VvNPR6MNm2phMeia5iHc6+GRDIbvL7DKIiS4nFx07kLIaL+v2lPOdCQP5wclDmfn3r1i1q5xTRqQTH+fg1jNGsmzHPjYXVjKmXxppiS6+Nb4/D723lr9/uhWA3PvPomdSPB6vH68/wNvL97C7tIbpEwYwtE9yswT94bp8JgzqRe/keAC8/gAj730PgPm3nMqYfqmUVNeRnpJw0P+Gh9NXW4qp8Pg4a2xDs1ggYHhtWR5nj+1LzyR7ncYYPN4AifHhx37c+a/l/GtpHu/cfArjBvY4LLG3RhO6Ui1orW09EDD7a9wAH60vwCGC3xhOG5XB3nIP7yzfw4b8CnaX1ZDocuJ0CIkuJx5vgF7JNmHvKKlmR0k1lbU+8str958vMzWBgopa0lPiuWHqEZx5ZF/ue2sVn248uDn5Jw3tzZo95ft7O2WmJhDnEHaXeXA6BH+TT5mjs3owbmAPkuOdHDekFze8uIwRmSn8deZE0hJd/O3jzbzwpV2ecVh6Mj0SXazaVcalxw/i/HH98Xj9TBuTyebCShLjnWT1SsLnD1BSXcc3O0rZWVLNVScO5astxUwa1ht3sMvsonUFnDoyo8UEGqqlf5+aOn+br68fn7D1d+cjIuworubhhet4d8UerjxhCD87ZzQJcQ7G3LcAgEuOy+IP3zum0TnW7innjrnLWbunnJSEOJbdd1bUv6loQleqkyir9lJcVUtyQhx909wUV9aS6nY1ShJlNV7yyz1UeLxsLqxic2ElU0dmkJbooqzGS11wPMPEQT3p38NNQUUtr369g8VbS0hzu7jrvDG8sSyPXaU1eLx+vtpiv1n0TUug0uNjSJ9kthVXUesLNEvyTYX7JtLS832S4ymt8TY6Z+/keEqqbDPZuIFpeLyB/QPsZk4azIfr8olzOBg7II3hGckkxDn5bGMhxw3pxcCeiTz7xTZOOiKd0X1T2FVaw9o9FXy1pRhfwBDvdHBJdha/+NaR/PE/G5izZCfXnjKMiyYORASmPvIRAD89axSXTRrEdc/nsDyvrFH8v/jWkfzm3bX79z/46VTqfAHG9Eul3OPjmF/9p1H55645ntNGZ/LNjn3sLfPwzso9PDD9KPoEv7XU1PkJGNPovkmo6jofpdVeBvRMbPX33hpN6Ep1c02bqwIBgzcQYGN+JS6ng+3FVUwY1JMdJdUUVtSSX+4hweXku8cOZNG6ApwOO11FcryTect3s624irV7KnC7HPgDhoLyWtJTEygPTja3o6SavqluVuSV4jeGYwf3Imf7PoyBI/un7v8WEu6bQ0eadcFYXHEO7n1zVaPjF00cyJvf7Nq/f9SANAKG/d18zzmqLwtX5wPwg5OG8twX2/aXPX98Py4+NovCilqe+XwrW4uqOGtsX+44azT/WrqTfmluzjmqHwlxDk77w0dUeHys/825BzS9RyhN6EqpqAsEzP4eS3X+ADtKqhnSJ4nCilr8AYNDhASXgz7JCSzZVrJ/Soz3Vu1ldL9U+qW5WbZjH9lDeuN0CGv2lJO7o5SCCg8TBvXkuCG9+N1769hXVcfAXolk9Upk2pi+vPL1Dkqr65gyKoPLJw3GbwwPvruWiYN7MnvRZkZkpPDklcfx/BfbeHjBOqaMzGBzYSUbCyoZnp7M6WMyuWHqESzZVsJLi7ezZJvtxQWNv4HU65fmZm+5p9Xfxd+vym7Utn8gDjmhi8i5wGOAE3jaGPNQk+cTgBeA44Bi4DJjzLbWzqkJXSnV2dQPZANYvbucIX2SSG0yyMzrD1Ba7SU+zkFqQhzr8ysor/HSKzmenkku0pMTWLh6L3vKPIwb2IM6X4Cc7SU4RBif1YMXvtjGNScPa7PHUksOKaGLiBPYAJwF5AFLgJnGmDUhZW4EjjbG3CAiM4CLjDGXtXZeTehKKXXgWkvokdyunQRsMsZsMcbUAa8CFzYpcyHwfHD7NeAM6aiRE0oppcKKJKEPBHaG7OcFj4UtY4zxAWVAn6YnEpHrRSRHRHIKCwsPLmKllFJhRZLQw9W0m7bTRFIGY8xTxphsY0x2RsbBtR8ppZQKL5KEngcMCtnPAna3VEZE4oAeQAlKKaUOm0gS+hJgpIgME5F4YAYwr0mZecDVwe1LgA9NtPpDKqVUN9XmikXGGJ+I3AQsxHZbfMYYs1pEHgByjDHzgH8A/xSRTdia+YyODFoppVRzES1BZ4yZD8xvcuz+kG0P8L32DU0ppdSB6J7zYSqlVBcUtaH/IlIIbD/Il6cDBzclXeej19I56bV0Pl3lOuDQrmWIMSZsN8GoJfRDISI5LY2UijV6LZ2TXkvn01WuAzruWrTJRSmlughN6Eop1UXEakJ/KtoBtCO9ls5Jr6Xz6SrXAR10LTHZhq6UUqq5WK2hK6WUakITulJKdRExl9BF5FwRWS8im0Tk7mjH0xYReUZECkRkVcix3iLyvohsDD72Ch4XEflL8NpWiMix0Yu8MREZJCKLRGStiKwWkVuDx2PxWtwi8rWILA9ey6+Cx4eJyOLgtcwJzl2EiCQE9zcFnx8azfjDERGniHwjIu8E92PyWkRkm4isFJFcEckJHovFv7GeIvKaiKwL/p858XBcR0wl9ODqSbOB84CxwEwRGRvdqNr0HHBuk2N3Ax8YY0YCHwT3wV7XyODP9cAThynGSPiAnxpjjgROAH4S/N3H4rXUAtOMMccAE4BzReQE4GHg0eC17AOuDZa/FthnjBkBPBos19ncCqwN2Y/lazndGDMhpJ92LP6NPQYsMMaMAY7B/tt0/HUYY2LmBzgRWBiyfw9wT7TjiiDuocCqkP31QP/gdn9gfXD7b9jl/ZqV62w/wFvYZQlj+lqAJGAZMBk7ci+u6d8admK6E4PbccFyEu3YQ64hK5ggpgHvYNcniNVr2QakNzkWU39jQBqwtenv9XBcR0zV0Ils9aRY0NcYswcg+JgZPB4T1xf8mj4RWEyMXkuwiSIXKADeBzYDpcauuAWN441oRa4o+jPwcyAQ3O9D7F6LAf4jIktF5PrgsVj7GxsOFALPBpvBnhaRZA7DdcRaQo9oZaQY1umvT0RSgNeB24wx5a0VDXOs01yLMcZvjJmArd1OAo4MVyz42GmvRUS+DRQYY5aGHg5TtNNfS9DJxphjsc0QPxGRKa2U7azXEgccCzxhjJkIVNHQvBJOu11HrCX0SFZPigX5ItIfIPhYEDzeqa9PRFzYZP6SMeaN4OGYvJZ6xphS4CPsfYGeYlfcgsbxduYVuU4GpovINuwC7tOwNfZYvBaMMbuDjwXAm9gP21j7G8sD8owxi4P7r2ETfIdfR6wl9EhWT4oFoSs8XY1tj64/flXwrvcJQFn9V7RoExHBLmSy1hjzp5CnYvFaMkSkZ3A7ETgTe9NqEXbFLWh+LZ1yRS5jzD3GmCxjzFDs/4cPjTFXEIPXIiLJIpJavw2cDawixv7GjDF7gZ0iMjp46AxgDYfjOqJ9A+EgbjicD2zAtnneG+14Ioj3FWAP4MV+El+LbbP8ANgYfOwdLCvYXjybgZVAdrTjD7mOU7BfA1cAucGf82P0Wo4Gvgleyyrg/uDx4cDXwCbgX0BC8Lg7uL8p+PzwaF9DC9d1GvBOrF5LMOblwZ/V9f+/Y/RvbAKQE/wb+zfQ63Bchw79V0qpLiLWmlyUUkq1QBO6Ukp1EZrQlVKqi9CErpRSXYQmdKWU6iI0oSulVBehCV0ppbqI/wcllvmMJuab+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation=\"relu\"))\n",
    "\n",
    "model.add(Dense(15, activation=\"relu\"))\n",
    "\n",
    "#Binary Classification\n",
    "model.add(Dense(1, activation= 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EarlyStopping in module tensorflow.python.keras.callbacks:\n",
      "\n",
      "class EarlyStopping(Callback)\n",
      " |  EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |  \n",
      " |  Stop training when a monitored quantity has stopped improving.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      monitor: Quantity to be monitored.\n",
      " |      min_delta: Minimum change in the monitored quantity\n",
      " |          to qualify as an improvement, i.e. an absolute\n",
      " |          change of less than min_delta, will count as no\n",
      " |          improvement.\n",
      " |      patience: Number of epochs with no improvement\n",
      " |          after which training will be stopped.\n",
      " |      verbose: verbosity mode.\n",
      " |      mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n",
      " |          training will stop when the quantity\n",
      " |          monitored has stopped decreasing; in `max`\n",
      " |          mode it will stop when the quantity\n",
      " |          monitored has stopped increasing; in `auto`\n",
      " |          mode, the direction is automatically inferred\n",
      " |          from the name of the monitored quantity.\n",
      " |      baseline: Baseline value for the monitored quantity.\n",
      " |          Training will stop if the model doesn't show improvement over the\n",
      " |          baseline.\n",
      " |      restore_best_weights: Whether to restore model weights from\n",
      " |          the epoch with the best value of the monitored quantity.\n",
      " |          If False, the model weights obtained at the last step of\n",
      " |          training are used.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  ```python\n",
      " |  callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
      " |  # This callback will stop the training when there is no improvement in\n",
      " |  # the validation loss for three consecutive epochs.\n",
      " |  model.fit(data, labels, epochs=100, callbacks=[callback],\n",
      " |      validation_data=(val_data, val_labels))\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EarlyStopping\n",
      " |      Callback\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_monitor_value(self, logs)\n",
      " |  \n",
      " |  on_epoch_end(self, epoch, logs=None)\n",
      " |      Called at the end of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: integer, index of epoch.\n",
      " |          logs: dict, metric results for this training epoch, and for the\n",
      " |            validation epoch if validation is performed. Validation result keys\n",
      " |            are prefixed with `val_`.\n",
      " |  \n",
      " |  on_train_begin(self, logs=None)\n",
      " |      Called at the beginning of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_end(self, logs=None)\n",
      " |      Called at the end of training.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Callback:\n",
      " |  \n",
      " |  on_batch_begin(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
      " |  \n",
      " |  on_batch_end(self, batch, logs=None)\n",
      " |      A backwards compatibility alias for `on_train_batch_end`.\n",
      " |  \n",
      " |  on_epoch_begin(self, epoch, logs=None)\n",
      " |      Called at the start of an epoch.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run. This function should only\n",
      " |      be called during TRAIN mode.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          epoch: integer, index of epoch.\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
      " |            number and the size of the batch.\n",
      " |  \n",
      " |  on_predict_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `predict` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Metric results for this batch.\n",
      " |  \n",
      " |  on_predict_begin(self, logs=None)\n",
      " |      Called at the beginning of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_predict_end(self, logs=None)\n",
      " |      Called at the end of prediction.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the beginning of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
      " |            number and the size of the batch.\n",
      " |  \n",
      " |  on_test_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a batch in `evaluate` methods.\n",
      " |      \n",
      " |      Also called at the end of a validation batch in the `fit`\n",
      " |      methods, if validation data is provided.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Metric results for this batch.\n",
      " |  \n",
      " |  on_test_begin(self, logs=None)\n",
      " |      Called at the beginning of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_test_end(self, logs=None)\n",
      " |      Called at the end of evaluation or validation.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          logs: dict. Currently no data is passed to this argument for this method\n",
      " |            but that may change in the future.\n",
      " |  \n",
      " |  on_train_batch_begin(self, batch, logs=None)\n",
      " |      Called at the beginning of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
      " |            number and the size of the batch.\n",
      " |  \n",
      " |  on_train_batch_end(self, batch, logs=None)\n",
      " |      Called at the end of a training batch in `fit` methods.\n",
      " |      \n",
      " |      Subclasses should override for any actions to run.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          batch: integer, index of batch within the current epoch.\n",
      " |          logs: dict. Metric results for this batch.\n",
      " |  \n",
      " |  set_model(self, model)\n",
      " |  \n",
      " |  set_params(self, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Callback:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(EarlyStopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor= 'val_loss', mode = 'min', patience= 25, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 1s 1ms/sample - loss: 0.6769 - val_loss: 0.6558\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.6293 - val_loss: 0.6124\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 86us/sample - loss: 0.5850 - val_loss: 0.5709\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.5433 - val_loss: 0.5285\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.5021 - val_loss: 0.4846\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 139us/sample - loss: 0.4588 - val_loss: 0.4404\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.4167 - val_loss: 0.3976\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.3759 - val_loss: 0.3576\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.3443 - val_loss: 0.3245\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 213us/sample - loss: 0.3119 - val_loss: 0.2920\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.2792 - val_loss: 0.2588\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.2529 - val_loss: 0.2365\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.2311 - val_loss: 0.2178\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.2187 - val_loss: 0.2047\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1994 - val_loss: 0.1908\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1875 - val_loss: 0.1812\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.1802 - val_loss: 0.1731\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1664 - val_loss: 0.1694\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1582 - val_loss: 0.1581\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1539 - val_loss: 0.1578\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1429 - val_loss: 0.1502\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1359 - val_loss: 0.1463\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.1289 - val_loss: 0.1472\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.1260 - val_loss: 0.1400\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1196 - val_loss: 0.1394\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1144 - val_loss: 0.1380\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1102 - val_loss: 0.1320\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1078 - val_loss: 0.1328\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1040 - val_loss: 0.1331\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0997 - val_loss: 0.1263\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0974 - val_loss: 0.1258\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0924 - val_loss: 0.1263\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0894 - val_loss: 0.1234\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0879 - val_loss: 0.1262\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.0858 - val_loss: 0.1233\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0831 - val_loss: 0.1230\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0809 - val_loss: 0.1222\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0804 - val_loss: 0.1263\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.0776 - val_loss: 0.1239\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.0769 - val_loss: 0.1276\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.0750 - val_loss: 0.1208\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.0742 - val_loss: 0.1215\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0737 - val_loss: 0.1262\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.0710 - val_loss: 0.1221\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0689 - val_loss: 0.1209\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.0689 - val_loss: 0.1239\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.0666 - val_loss: 0.1197\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.0670 - val_loss: 0.1207\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.0737 - val_loss: 0.1289\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.0648 - val_loss: 0.1212\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0635 - val_loss: 0.1201\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0634 - val_loss: 0.1199\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0623 - val_loss: 0.1222\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0624 - val_loss: 0.1219\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0668 - val_loss: 0.1162\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0606 - val_loss: 0.1222\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0602 - val_loss: 0.1242\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0582 - val_loss: 0.1154\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0595 - val_loss: 0.1245\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0612 - val_loss: 0.1197\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 85us/sample - loss: 0.0585 - val_loss: 0.1216\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0581 - val_loss: 0.1217\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 85us/sample - loss: 0.0566 - val_loss: 0.1217\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0561 - val_loss: 0.1185\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0586 - val_loss: 0.1253\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0564 - val_loss: 0.1194\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0583 - val_loss: 0.1198\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0548 - val_loss: 0.1216\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0557 - val_loss: 0.1203\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.0557 - val_loss: 0.1239\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0554 - val_loss: 0.1237\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0569 - val_loss: 0.1214\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0566 - val_loss: 0.1262\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0530 - val_loss: 0.1205\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.0537 - val_loss: 0.1265\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0517 - val_loss: 0.1218\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.0536 - val_loss: 0.1190\n",
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.0549 - val_loss: 0.1249\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.0517 - val_loss: 0.1213\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.0530 - val_loss: 0.1272\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0516 - val_loss: 0.1272\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0520 - val_loss: 0.1262\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.0548 - val_loss: 0.1228\n",
      "Epoch 00083: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18645e205c8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x= X_train, y = y_train, epochs = 600, validation_data = (X_test, y_test), callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_df = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18645eb8808>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(15, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Binary Classification\n",
    "model.add(Dense(1, activation= 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 1s 1ms/sample - loss: 0.7127 - val_loss: 0.6826\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.6883 - val_loss: 0.6626\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.6785 - val_loss: 0.6435\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.6581 - val_loss: 0.6247\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.6302 - val_loss: 0.6003\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 114us/sample - loss: 0.6201 - val_loss: 0.5730\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 130us/sample - loss: 0.5981 - val_loss: 0.5458\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 108us/sample - loss: 0.5843 - val_loss: 0.5219\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 119us/sample - loss: 0.5787 - val_loss: 0.4998\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.5473 - val_loss: 0.4735\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 120us/sample - loss: 0.5194 - val_loss: 0.4483\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 115us/sample - loss: 0.4944 - val_loss: 0.4195\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 110us/sample - loss: 0.4830 - val_loss: 0.3953\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.4790 - val_loss: 0.3788\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.4394 - val_loss: 0.3569\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.4186 - val_loss: 0.3327\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 115us/sample - loss: 0.4044 - val_loss: 0.3084\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 128us/sample - loss: 0.4168 - val_loss: 0.2981\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 128us/sample - loss: 0.3702 - val_loss: 0.2786\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.3894 - val_loss: 0.2672\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 100us/sample - loss: 0.3665 - val_loss: 0.2570\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.3318 - val_loss: 0.2410\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.3349 - val_loss: 0.2386\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.3139 - val_loss: 0.2259\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 112us/sample - loss: 0.3215 - val_loss: 0.2171\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.3180 - val_loss: 0.2048\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.3008 - val_loss: 0.1994\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.2964 - val_loss: 0.1886\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 124us/sample - loss: 0.2864 - val_loss: 0.1826\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 84us/sample - loss: 0.2702 - val_loss: 0.1753\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 112us/sample - loss: 0.2673 - val_loss: 0.1715\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.2487 - val_loss: 0.1629\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 112us/sample - loss: 0.2335 - val_loss: 0.1590\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.2523 - val_loss: 0.1531\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.2464 - val_loss: 0.1508\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.2428 - val_loss: 0.1451\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 131us/sample - loss: 0.2444 - val_loss: 0.1425\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 123us/sample - loss: 0.2283 - val_loss: 0.1398\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 131us/sample - loss: 0.2206 - val_loss: 0.1430\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 130us/sample - loss: 0.2206 - val_loss: 0.1290\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 119us/sample - loss: 0.2169 - val_loss: 0.1242\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 135us/sample - loss: 0.2073 - val_loss: 0.1254\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 131us/sample - loss: 0.2199 - val_loss: 0.1229\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 120us/sample - loss: 0.2140 - val_loss: 0.1176\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 143us/sample - loss: 0.2000 - val_loss: 0.1209\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.2187 - val_loss: 0.1165\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.1860 - val_loss: 0.1165\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 127us/sample - loss: 0.1901 - val_loss: 0.1140\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 117us/sample - loss: 0.1739 - val_loss: 0.1121\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 130us/sample - loss: 0.2167 - val_loss: 0.1088\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 123us/sample - loss: 0.1873 - val_loss: 0.1099\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 237us/sample - loss: 0.1891 - val_loss: 0.1133\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 211us/sample - loss: 0.1775 - val_loss: 0.1104\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 261us/sample - loss: 0.1655 - val_loss: 0.1103\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 197us/sample - loss: 0.2052 - val_loss: 0.1094\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 183us/sample - loss: 0.1740 - val_loss: 0.1137\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 235us/sample - loss: 0.1647 - val_loss: 0.1173\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 164us/sample - loss: 0.1712 - val_loss: 0.1028\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 193us/sample - loss: 0.1464 - val_loss: 0.1111\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.1505 - val_loss: 0.1018\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1712 - val_loss: 0.1024\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1679 - val_loss: 0.1001\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.1481 - val_loss: 0.0996\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1362 - val_loss: 0.0992\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 132us/sample - loss: 0.1600 - val_loss: 0.0993\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1623 - val_loss: 0.0933\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 117us/sample - loss: 0.1488 - val_loss: 0.0989\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 119us/sample - loss: 0.1485 - val_loss: 0.0953\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.1447 - val_loss: 0.0979\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 123us/sample - loss: 0.1460 - val_loss: 0.0933\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 108us/sample - loss: 0.1388 - val_loss: 0.0974\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 108us/sample - loss: 0.1290 - val_loss: 0.0962\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1335 - val_loss: 0.0936\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 112us/sample - loss: 0.1346 - val_loss: 0.0991\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1364 - val_loss: 0.0913\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 104us/sample - loss: 0.1271 - val_loss: 0.0973\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 131us/sample - loss: 0.1269 - val_loss: 0.0963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1166 - val_loss: 0.0996\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 104us/sample - loss: 0.1131 - val_loss: 0.0956\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1310 - val_loss: 0.0914\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1155 - val_loss: 0.0882\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 112us/sample - loss: 0.1283 - val_loss: 0.1087\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1325 - val_loss: 0.0885\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1104 - val_loss: 0.1033\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 121us/sample - loss: 0.1413 - val_loss: 0.1025\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 115us/sample - loss: 0.1269 - val_loss: 0.0917\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1206 - val_loss: 0.0937\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1218 - val_loss: 0.0946\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1092 - val_loss: 0.0970\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1194 - val_loss: 0.0978\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1216 - val_loss: 0.0946\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1152 - val_loss: 0.0892\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1091 - val_loss: 0.0900\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1115 - val_loss: 0.0879\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1006 - val_loss: 0.0885\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 98us/sample - loss: 0.1256 - val_loss: 0.0871\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 100us/sample - loss: 0.1313 - val_loss: 0.0882\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1235 - val_loss: 0.0883\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.0911 - val_loss: 0.0848\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.1187 - val_loss: 0.0888\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 94us/sample - loss: 0.1194 - val_loss: 0.0993\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 97us/sample - loss: 0.1142 - val_loss: 0.1003\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 104us/sample - loss: 0.0993 - val_loss: 0.0852\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 98us/sample - loss: 0.1245 - val_loss: 0.0932\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1183 - val_loss: 0.1004\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 100us/sample - loss: 0.1098 - val_loss: 0.0869\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.1054 - val_loss: 0.0850\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 95us/sample - loss: 0.0906 - val_loss: 0.0953\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 101us/sample - loss: 0.1056 - val_loss: 0.0933\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1031 - val_loss: 0.0900\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 96us/sample - loss: 0.1227 - val_loss: 0.0905\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 104us/sample - loss: 0.1034 - val_loss: 0.0929\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 104us/sample - loss: 0.1053 - val_loss: 0.0953\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 108us/sample - loss: 0.1059 - val_loss: 0.0920\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 106us/sample - loss: 0.0945 - val_loss: 0.0962\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 118us/sample - loss: 0.0940 - val_loss: 0.0976\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 103us/sample - loss: 0.1218 - val_loss: 0.0843\n",
      "Epoch 118/600\n",
      "426/426 [==============================] - 0s 122us/sample - loss: 0.0788 - val_loss: 0.0916\n",
      "Epoch 119/600\n",
      "426/426 [==============================] - 0s 110us/sample - loss: 0.0887 - val_loss: 0.0851\n",
      "Epoch 120/600\n",
      "426/426 [==============================] - 0s 126us/sample - loss: 0.1037 - val_loss: 0.1017\n",
      "Epoch 121/600\n",
      "426/426 [==============================] - 0s 107us/sample - loss: 0.1006 - val_loss: 0.0926\n",
      "Epoch 122/600\n",
      "426/426 [==============================] - 0s 109us/sample - loss: 0.1020 - val_loss: 0.0971\n",
      "Epoch 123/600\n",
      "426/426 [==============================] - 0s 119us/sample - loss: 0.0882 - val_loss: 0.0953\n",
      "Epoch 124/600\n",
      "426/426 [==============================] - 0s 110us/sample - loss: 0.1158 - val_loss: 0.1001\n",
      "Epoch 125/600\n",
      "426/426 [==============================] - 0s 113us/sample - loss: 0.0990 - val_loss: 0.0990\n",
      "Epoch 126/600\n",
      "426/426 [==============================] - 0s 115us/sample - loss: 0.1044 - val_loss: 0.0901\n",
      "Epoch 127/600\n",
      "426/426 [==============================] - 0s 112us/sample - loss: 0.0991 - val_loss: 0.0970\n",
      "Epoch 128/600\n",
      "426/426 [==============================] - 0s 167us/sample - loss: 0.1029 - val_loss: 0.0942\n",
      "Epoch 129/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.1219 - val_loss: 0.1157\n",
      "Epoch 130/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.0810 - val_loss: 0.0928\n",
      "Epoch 131/600\n",
      "426/426 [==============================] - 0s 151us/sample - loss: 0.1262 - val_loss: 0.1097\n",
      "Epoch 132/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.0908 - val_loss: 0.0968\n",
      "Epoch 133/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.0814 - val_loss: 0.0929\n",
      "Epoch 134/600\n",
      "426/426 [==============================] - 0s 225us/sample - loss: 0.0929 - val_loss: 0.0898\n",
      "Epoch 135/600\n",
      "426/426 [==============================] - 0s 169us/sample - loss: 0.0941 - val_loss: 0.1100\n",
      "Epoch 136/600\n",
      "426/426 [==============================] - 0s 152us/sample - loss: 0.0890 - val_loss: 0.0958\n",
      "Epoch 137/600\n",
      "426/426 [==============================] - 0s 130us/sample - loss: 0.0944 - val_loss: 0.0929\n",
      "Epoch 138/600\n",
      "426/426 [==============================] - 0s 150us/sample - loss: 0.0919 - val_loss: 0.1000\n",
      "Epoch 139/600\n",
      "426/426 [==============================] - 0s 161us/sample - loss: 0.0823 - val_loss: 0.1001\n",
      "Epoch 140/600\n",
      "426/426 [==============================] - 0s 131us/sample - loss: 0.0848 - val_loss: 0.0918\n",
      "Epoch 141/600\n",
      "426/426 [==============================] - 0s 160us/sample - loss: 0.0974 - val_loss: 0.0981\n",
      "Epoch 142/600\n",
      "426/426 [==============================] - 0s 141us/sample - loss: 0.0789 - val_loss: 0.1050\n",
      "Epoch 00142: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1864621d188>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x= X_train, y = y_train, epochs = 600, validation_data = (X_test, y_test), callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x18647473f88>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3hUVf7H8feZSS8kIZU0kkBCDQQITbpSRcGCCKKuXde1rK5t113Xte5PXeuyKvaGgIiKggLSkRoglAAJIZCQBEgjhYTUOb8/boAEAkxwkiHh+3qePHHuvXPnOwifOXPuuecorTVCCCFaPpO9CxBCCGEbEuhCCNFKSKALIUQrIYEuhBCthAS6EEK0Eg72emE/Pz8dERFhr5cXQogWafPmzXlaa/+G9tkt0CMiIkhISLDXywshRIuklEo/2z7pchFCiFZCAl0IIVoJCXQhhGgl7NaHLoS4NFVVVZGZmUl5ebm9S7moubi4EBoaiqOjo9XPkUAXQjSrzMxMPD09iYiIQCll73IuSlpr8vPzyczMJDIy0urnSZeLEKJZlZeX4+vrK2F+DkopfH19G/0tRgJdCNHsJMzP70L+jFpcoG9OP8r//bLH3mUIIcRFp8UFelJ2Ee+u2Mf+vFJ7lyKEaKE8PDzsXUKTsCrQlVJjlVLJSqlUpdRTDex/QymVWPuTopQqtH2phuExAQCsSM5pqpcQQogW6byBrpQyA9OBcUBXYKpSqmvdY7TWj2it47TWccA7wLymKBYg3NeNKD93ViTnNtVLCCEuEVprHn/8cbp3705sbCyzZ88G4NChQwwdOpS4uDi6d+/O6tWrqamp4bbbbjt57BtvvGHn6s9kzbDFfkCq1joNQCk1C5gI7DrL8VOBf9qmvIYN6+TPzA0ZlFfV4OJobsqXEkI0oX/9mMSu7GKbnrNrcBv+eXU3q46dN28eiYmJbNu2jby8PPr27cvQoUOZOXMmY8aM4emnn6ampoaysjISExPJyspi586dABQWNllHxAWzpsslBDhY53Fm7bYzKKXaA5HAsrPsv0cplaCUSsjNvfAW9vBOAVRUW1ifln/B5xBCiDVr1jB16lTMZjOBgYEMGzaMTZs20bdvXz755BOeffZZduzYgaenJ1FRUaSlpfHggw/yyy+/0KZNG3uXfwZrWugNjZ0528rSU4C5WuuahnZqrWcAMwDi4+MveHXq/pFtcXYwsSI5l+GdAi70NEIIO7O2Jd1UtG44hoYOHcqqVatYsGABt9xyC48//ji33nor27ZtY9GiRUyfPp05c+bw8ccfN3PF52ZNCz0TCKvzOBTIPsuxU4Cvf29R5+PiaGZgB19Wpkg/uhDiwg0dOpTZs2dTU1NDbm4uq1atol+/fqSnpxMQEMDdd9/NnXfeyZYtW8jLy8NisXD99dfz/PPPs2XLFnuXfwZrWuibgGilVCSQhRHaN51+kFKqE+ADrLNphWcxPMafZ3/cRVJ2Ed2CvZrjJYUQrcy1117LunXr6NmzJ0opXnnlFYKCgvjss8949dVXcXR0xMPDg88//5ysrCxuv/12LBYLAC+//LKdqz+TOttXjnoHKXUl8CZgBj7WWr+olHoOSNBaz6895lnARWt9xrDGhsTHx+sLWuDCYoG8ZArcOzD6jZX4eTjzwwODcHaQi6NCtAS7d++mS5cu9i6jRWjoz0optVlrHd/Q8VaNQ9daL9Rax2itO2itX6zd9syJMK99/Ky1Yf67rPw3vDeEtuoYr0zqwZ7DJby2KLnJX1YIIS52Le5OUTqNA0sV7PqByzsHcvOAcD5YvZ/N6UftXZkQQthVywv0dnHgGw07vgHgb1d2wcPZgTmbDp7niUII0bq1vEBXCnpMhvTfoCgTNycHRnYJ4Jekw1TVWOxdnRBC2E3LC3SA2EnG7x1zAbiqRzBFx6tYk5pnx6KEEMK+Wmagt42C0L4nu12GxPjh6eLAT9sO2bkwIYSwn5YZ6ACxN8CRnXBkF84OZkZ3DWLxrsNUVDd4k6oQQrR6LTfQu10HynyylX5Vz3aUlFezOkW6XYQQtnOuudMPHDhA9+7dm7Gac2u5ge7hDx1GGP3oFguDO/rh6ezA0j1H7F2ZEELYhTW3/l+8YifDd/fAwQ04th9Iv8i2bEgrsHdVQghr/fwUHN5h23MGxcK4f59195NPPkn79u25//77AXj22WdRSrFq1SqOHj1KVVUVL7zwAhMnTmzUy5aXl/PHP/6RhIQEHBwceP311xkxYgRJSUncfvvtVFZWYrFY+PbbbwkODmby5MlkZmZSU1PDP/7xD2688cbf9bahpQd65/Hg6AY75kD7gfSPasvSPTnkFJcT0MbF3tUJIS5CU6ZM4c9//vPJQJ8zZw6//PILjzzyCG3atCEvL48BAwYwYcKERi3UPH36dAB27NjBnj17GD16NCkpKbz33ns8/PDDTJs2jcrKSmpqali4cCHBwcEsWLAAgKKiIpu8t5Yd6M4e0OlKSPoOxv4fA6J8AVi/v4AJPYPtXJwQ4rzO0ZJuKr169SInJ4fs7Gxyc3Px8fGhXbt2PPLII6xatQqTyURWVhZHjhwhKCjI6vOuWbOGBx98EIDOnTvTvn17UlJSGDhwIC+++CKZmZlcd911REdHExsby2OPPcaTTz7JVVddxZAhQ2zy3lpuH/oJsTfA8aOwbyld27XB09lBFr4QQpzTpEmTmDt3LrNnz2bKlCl89dVX5ObmsnnzZhITEwkMDKS8vLxR5zzbRIc33XQT8+fPx9XVlTFjxrBs2TJiYmLYvHkzsbGx/PWvf+W5556zxdtqBYHe8Qpw84Nts3Awm4iP8JFAF0Kc05QpU5g1axZz585l0qRJFBUVERAQgKOjI8uXLyc9Pb3R5xw6dChfffUVACkpKWRkZNCpUyfS0tKIiorioYceYsKECWzfvp3s7Gzc3Ny4+eabeeyxx2w2t3rL7nIBMDsad44mfAzHjzIgypflybnklJQT4Cn96EKIM3Xr1o2SkhJCQkJo164d06ZN4+qrryY+Pp64uDg6d+7c6HPef//93HfffcTGxuLg4MCnn36Ks7Mzs2fP5ssvv8TR0ZGgoCCeeeYZNm3axOOPP47JZMLR0ZF3333XJu/LqvnQm8IFz4fekOytMGM4XPUGiYHXcc3033hnai+uln50IS46Mh+69ZpkPvSLXrs48O8C22bRPbgN7k5mWZ5OCHHJaR2BrhT0nAIHN+BQuJ9reoUwb0umzJEuhLCJHTt2EBcXV++nf//+9i7rDC2/D/2EHpPh12dhxzc8Ne4vrEjO5bFvtrHwoSG4OsnydEJcTLTWjRrjbW+xsbEkJiY262teSHd462ihA7QJhtB42LsYTxdHXr2hB/vzSnlVlqcT4qLi4uJCfn7+BQXWpUJrTX5+Pi4ujRvY0Xpa6AAdR8KKf0NpHpd18GNSn1C+3pjBE2M74eIorXQhLgahoaFkZmaSmyvXuc7FxcWF0NDQRj2nlQX6KFjxMuxbDj1uYGJcMHM3Z7J6bx6jugbauzohBODo6EhkZKS9y2iVWk+XC0BwL3DzhdQlAAyI8sXTxYFFSYftXJgQQjQ9qwJdKTVWKZWslEpVSj11lmMmK6V2KaWSlFIzbVumlUwm6HA5pC4FiwVHs4krOgewdPcRqmW9USFEK3feQFdKmYHpwDigKzBVKdX1tGOigb8Cg7TW3YA/N0Gt1uk4Csry4JBxRXpMtyCOllWx8YBMqyuEaN2saaH3A1K11mla60pgFnD6RMF3A9O11kcBtNY5ti2zETpeASijlQ4M6+SPs4OJxUmy8IUQonWzJtBDgIN1HmfWbqsrBohRSv2mlFqvlBrb0ImUUvcopRKUUglNdoXb3Q/a9YR9ywBwc3JgSLQ/i5MOU2ORYVJCiNbLmkBvaPT/6cnoAEQDw4GpwIdKKe8znqT1DK11vNY63t/fv7G1Wi9yKGRugsoyAK7vHUJ2UTk/bc9uutcUQgg7sybQM4GwOo9DgdOTMRP4QWtdpbXeDyRjBLx9RA4FSxUc3AAY/eidAj15e+leaaULIVotawJ9ExCtlIpUSjkBU4D5px3zPTACQCnlh9EFk2bLQhslfAAoMxxYDYDJpHh4ZDT7ckullS6EaLXOG+ha62rgAWARsBuYo7VOUko9p5SaUHvYIiBfKbULWA48rrW23yoTzp4Q0hv2rz65aWy3IDoHGa30iuoau5UmhBBNxapx6FrrhVrrGK11B631i7XbntFaz6/9b621flRr3VVrHau1ntWURVslYghkb4GKY4DRSn9sdCf25ZYydcZ6coobt7yUEEJc7FrXnaJ1RQ4BSzVkrD+5aWTXQP43rTd7Dpdw1TtryC48bscChRDCtlpvoIf1B5MjHFhVb/OVse2Ydc8Ackoq+GWnTAkghGg9Wm+gO7lDSJ96/egn9Aj1JtjLha0HC+1QmBBCNI3WG+hgdLscSoTyojN29Qr3YWuGrGgkhGg9WnegRwwBbYH0dWfs6hXuTebR4+SUyMVRIUTr0LoDPawfmJ1Ojkevq1e4DwBbM6TbRQjROrTuQHd0hdB+sH/VGbu6h7TByWySQBdCtBqtO9DB6Ec/vAPK6k+f6+xgpmtwG7ZIP7oQopVo/YEeMQTQkL72jF29wr3Znlkoi18IIVqF1h/oofHg4NJgP3rvcB/KqyzsOVxih8KEEMK2Wn+gOzgbNxk1MB69V7gxw690uwghWoPWH+gAUcMgJwlK6q9aFOLtSjsvFzbul+XphBAt36UR6B1HGb9Tf623WSnFgChf1qcVoLXMky6EaNkujUAPigWPINi7+Ixd/SPbknesgn25pXYoTAghbOfSCHSlIHoU7FsONVX1dg2I8gVgfZr9pm8XQghbuDQCHSB6NFQUnVyW7oT2vm4EtnFmg/SjCyFauEsn0KOGG9PpntbtcqofPV/60YUQLdqlE+gubaD9QNi75Ixd/SN9yS2pIC1P+tGFEC3XpRPoYHS75OyCwox6mwdEtQVgQ5p0uwghWq5LK9A7XWn8Tv653uZIP3cCPJ1Zuy/PDkUJIYRtXFqB7tsB/DvD7h/rbVZKMTjajzWpedRYpB9dCNEyXVqBDtB5vDFR12mzLw6L8aewrIodWWeubiSEEC2BVYGulBqrlEpWSqUqpZ5qYP9tSqlcpVRi7c9dti/VRjqPB10DKYvqbR7c0Q+lYFVKrp0KE0KI3+e8ga6UMgPTgXFAV2CqUqprA4fO1lrH1f58aOM6bSe4N3gGw56f6m329XCme7CXBLoQosWypoXeD0jVWqdprSuBWcDEpi2rCSlltNJTl0JlWb1dQ2P82HqwkOLyqrM8WQghLl7WBHoIcLDO48zabae7Xim1XSk1VykVZpPqmkrn8VB9HNJW1Ns8NNqfGotmbaqMdhFCtDzWBLpqYNvpQ0F+BCK01j2AX4HPGjyRUvcopRKUUgm5uXbs2mh/GTi4nhHovdv74OHswMoUCXQhRMtjTaBnAnVb3KFAdt0DtNb5WuuK2ocfAH0aOpHWeobWOl5rHe/v738h9dqGg7MR6vtX1tvsaDYxvJM/P27L5khxuZ2KE0KIC2NNoG8CopVSkUopJ2AKML/uAUqpdnUeTgB2267EJhI1HHL3QPGhepsfG92JyhoLLy64+N+CEELUdd5A11pXAw8AizCCeo7WOkkp9ZxSakLtYQ8ppZKUUtuAh4Dbmqpgm4kabvw+rZUe4efOfcM6MH9btvSlCyFaFGWvGQbj4+N1QkKCXV4bAIsFXutozO9y7Xv1dpVX1TDqjZVYLPDqpB5c1tHPTkUKIUR9SqnNWuv4hvZdeneKnmAyQeRQ48LoaR9qLo5m3prSC5MJbvpwAw99vVWmBBBCXPQu3UAHo9ul5BDk7T1jV+9wH5Y8MozbLotg/rZskg+XNHt5QgjRGBLocMbwxRNcHM3c1D8cgOQjxc1SkhBCXKhLO9B9IoyfswQ6GFPrOpoVe6SFLoS4yF3agQ5GK/3AaqipbnC3o9lEB38PUiTQhRAXOQn0qOFQUQzZW896SKcgT+lDF0Jc9CTQI4Yav8/R7dIpyJPsonKKjsukXUKIi5cEursvBPU44wajujoHeQKQckRa6UKIi5cEOhjdLgc3QGVpg7s7BbUBkG4XIcRFTQIdjECvqYSMdQ3uDvZywdPZQQJdCHFRk0AHCB8IZqez9qMrpYiRC6NCiIucBDqAkxuE9T/vhdE9h4ux19w3QghxPhLoJ0QNh8M7oLThGRY7B3lSXF7NkeKKBvcLIYS9SaCfEDXc+L1/VYO7u7QzLoxuzTjaPPUIIUQjSaCf0C4OnL3O2u3SK8ybtu5O/LzzcPPWJYQQVpJAP8HsAJFDzhroDmYTY7oF8uvuI5RX1TRvbUIIYQUJ9LqihkNhOhTsb3D3+NhgyiprWJFsxwWuhRDiLCTQ64oabvw+Syt9QFRb2ro7sXDHoQb3CyGEPUmg1+XbEdqEnHUaAOl2EUJczCTQ61IKIodB2kpjzdEGnOh2WbYn5+S2gtJKjpZWNleVQgjRIAn000UNh+MFcGRHg7sHRLUlxNuVGavS0FpTWW1h0rtr+fPsxGYtUwghTieBfrqoYcbvc4x2+ePwDiQeLOS31Hy+2pBOWl4pew7LEnVCCPuSQD+dZxD4dznnNAA3xIcS1MaFVxcn89bSvZhNiiPFFZRWNLzqkRBCNAerAl0pNVYplayUSlVKPXWO4yYppbRSKt52JdpB1HBIXwfVDd/m7+xg5t5hUWw7WEjR8Sr+NLwDAPvzGp5+VwghmsN5A10pZQamA+OArsBUpVTXBo7zBB4CNti6yGYXNRyqj8PBjWc9ZErfcIK9XJjSN5wre7QDJNCFEPblYMUx/YBUrXUagFJqFjAR2HXacc8DrwCP2bRCe4gYBMpsdLtEDmnwEFcnM8seG46j2URVjTEiRgJdCGFP1nS5hAAH6zzOrN12klKqFxCmtf7pXCdSSt2jlEpQSiXk5l7Ed1s6e0JoPKQtP+dhLo5mzCaFi6OZEG9XCXQhhF1ZE+iqgW0nJwVXSpmAN4C/nO9EWusZWut4rXW8v7+/9VXaQ/QoyNoC+fusOjzSz500CXQhhB1ZE+iZQFidx6FAdp3HnkB3YIVS6gAwAJjf4i+M9roFTGZI+NiqwyP93NmfewytNbsPFfPs/CRqLLIYhhCi+VgT6JuAaKVUpFLKCZgCzD+xU2tdpLX201pHaK0jgPXABK11QpNU3Fw8g6DL1bD1C6gsO+/hkX7uFJdXU1BayfTlqXy69gCJB2XudCFE8zlvoGutq4EHgEXAbmCO1jpJKfWcUmpCUxdoV33vhvIi2Dn3vIdG+rsDsD2ziCW7jgCwZFfOuZ4ihBA2ZdU4dK31Qq11jNa6g9b6xdptz2it5zdw7PAW3zo/of1lENAVNs6A86wlGuVnBPq7K/ZRUW0hsI0zS3cfaY4qhRACkDtFz00p6Hunsdbo4YbndjkhxNsVR7Ni44ECInzduGdoB/bmHCM9Xy6UCiGahwT6+XSZCMoEe845IhMHs4nwtm4AXNMrhFFdAgH4dbd0uwghmocE+vl4+EPYANh97kAHiPTzAOCauBDCfd2ICfSQbhchRLORQLdGl6sgJwkK0s552A3xodw7LIqI2v70K7oEsnF/AUXHq5qjSiHEJU4C3RqdrzJ+n6eVPqZbEH8d1+Xk42Ex/lRbNJvTC5qyOiGEACTQrePTHoJiz9uPfrrYEC+Ugh2ZMle6EKLpSaBbq/PVxuyLJdb3ibs7OxDl586OrKImLEwIIQwS6NbqPB7QsHdxo54WG+LFTgl0IUQzkEC3VmA38AyG1CWNelr3EC8OF5eTW9LwYhlCCGErEujWUgqiR8K+FVBj/VJzsSFeANJKF0I0OQn0xug4CiqKIPPsKxmdrtuJC6MS6EKIJiaB3hhRw8Hk0Kh+dA9nByLlwqgQohlIoDeGSxvjrtG9vzbqaXJhVAjRHCTQGyt6JBzZAcWHrH5KbIgXh4rKWbbnCH//fgcJB+RGIyGE7UmgN1bHUcbvVOtb6d1rL4ze8WkCX67P4O1lqU1RmRDiEudg7wJanLrDF3vfYtVT4sK8ub53KN2C23Agv5SZGzIoLKvE282piYsVQlxKpIXeWEpBxysaNXzRxdHMfyb35I7BkUzqE0q1RbN4l8zCKISwLQn0CxHd+OGLJ8SGeBHq48rPO4w++M3pBczamEF5VY2tqxRCXGIk0C9E1PDa4YuNu2sUQCnFlbHtWJOax6qUXG7+cCNPzdvBiNdWMG9Lps1LFUJcOiTQL4SLF4T1b/Q0ACeM6x5EVY3mtk824u/pzHs39yagjQuPztlGWu4xGxcrhLhUSKBfqI4jjXVGSw43+qlxYd6E+rjS1t2JL+7sx9ju7fjglj44mBSzNh1sgmKFEJcCCfQLFd344YsnKKWYedcAFjw0hPa+xupGAW1cGNU1kG8SDkp/uhDiglgV6EqpsUqpZKVUqlLqqQb236eU2qGUSlRKrVFKdbV9qReZwO7g3R7WvgNV5Y1+erivG4FtXOptu6l/OEfLqliU1PhWvxBCnDfQlVJmYDowDugKTG0gsGdqrWO11nHAK8DrNq/0YqMUjH8dcvfAipdscspBHfwIb+vGVxsybHI+IcSlxZoWej8gVWudprWuBGYBE+seoLWuu8aaO6BtV+JFLHok9P6D0Uo/2PghjKczmRQ39Q9n4/4CXl64m+OV0vUihLCeNYEeAtS9UpdZu60epdSflFL7MFroDzV0IqXUPUqpBKVUQm5u7oXUe/EZ/QK0CYFFT9vkdLddFsHk+FDeX5XGmDdXyagXIYTVrAl01cC2M1rgWuvpWusOwJPA3xs6kdZ6htY6Xmsd7+/v37hKL1YubaDvXcZNRgVpv/90jmZemdSTr+8eQHF5FX+enUhVjcUGhQohWjtrAj0TCKvzOBTIPsfxs4Brfk9RLU7sJEDBjrk2O+XADr68dG0s2zOL+K9M5iWEsII1gb4JiFZKRSqlnIApwPy6Byilous8HA/stV2JLYBXKLQfBNvngLbd5YMrY9txba8Q/rs8lW0HC212XiFE63TeQNdaVwMPAIuA3cAcrXWSUuo5pdSE2sMeUEolKaUSgUeBPzRZxRerHjdA/l44lGjT0z47oRvero68vfTS+owUQjSeVdPnaq0XAgtP2/ZMnf9+2MZ1tTxdJ8KCx2D7NxDcy2an9XJ1ZFr/cN5Znkp6funJG5GEEOJ0cqeorbj6QMwY2D4Ljtu2e2TagPaYleLzdek2Pa8QonWRQLeloY/B8aOw3DY3Gp0Q2MaFsd2DmJNwkNIK6+ZgF0JceiTQbSm4F8TfCZs+gGzb9qXfPiiCkvJq3l2xj4pqueFICHEmCXRbu/zv4OYLC/4CFtsFb+9wH4bG+PPf5akM+vcy5iTIrIxCiPok0G3N1RvGvARZCbD+XZudVinFp7f15fM7+hHi7cqz85Moq5TuFyHEKRLoTSH2Bug0HpY+B7nJNjutyaQYGuPP0+O7UlZZwy87ZVZGIcQpEuhNQSm4+k1wcofv7rV6MWlr9Y3wIbytG9/KknVCiDok0JuKRwCMfw2yt0LSPJueWinFdb1DWLsvn+zC4zY9txCi5ZJAb0pdrwXfaKMv3YZTAgBc3zsUreG7rVknt2XklzHurdX8kJh1jmcKIVorCfSmZDJB/3shewtkJtj01GFt3egX2Za5mzOxWIwPi683ZbD7UDEPz0rkjSUpaBt/iAghLm4S6E2t51RwbgMbbDfi5YSp/cLYn1fKipQcaiya77dmMSTaj0l9Qnlr6V6ue3cta/flkZZ7jDkJB9mScdTmNQghLh5WzeUifgdnD+h1C2x837jZKDjOZqe+qkcwr/ySzPsr03AymzlUVM7T47swPrYd/SLb8saSFG76YMPJ4wM8nVnz5OU4OcjnuBCtkQR6c+h/D2z5DGYMg6BYYy3SsH6/+7SOZhN3DIrkxYW7Kf1lN54uDozsEohSisnxYUzoGcy8LUZ/ukbz9Hc7+XnnISbGnbHglBCiFZCmWnPwiYAHt8DYf0NZAXz/R6ipssmpp/QLw9PZgZ1ZxVzVIxgXR/PJfS6OZm7qH85N/cOZ2jecSD93Pl17wCavK4S4+EigNxfPQBjwR7jyNchPNVrstjitiyM3DQgH4PreZ295m0yKPwxsz9aMQhJlsQwhWiUJ9ObWaZyxutGKf0NFiU1O+ecrYvjw1nj6tPc553GT4sPwcHbg4zX7bfK6QoiLiwR6c1MKRj0Ppbmw+j82OaWrk5mRXY2+83PxcHZgWv9w5m/LZu5muctUiNZGAt0eQvtA3DRY8yak/tqsL/2X0Z0Y1NGXp77dztLdRyg6XiWTfAnRSih73XwSHx+vExJse7NNi1JZCh+OgpJsuGcl+LRvtpcuKa9i8vvr2X2oGDC+NFzXK5RHR8cQ4u168riXFu6mqsbCX8d1qTfUsbyqhkVJh5nQM/i83wqEELallNqstY5vaJ+00O3FyR1u/AIsFph7u/G7mXi6OPLVXf159uqu/OOqrvxhYAQ/bs9mxGsrWLLrCACbDhQwY1Uan/x2gFs/3kBhWeXJ53++7gAPz0pk04Gz36j0xboDvPLLniZ+J0KIuiTQ7cm3A1z5KmRthu2zm/Wl27o7cdugSO4cHMmzE7qx/LHhxAR68Ng328guPM5LC3cT2MaZl6+LZUt6Ibd+vBGLRaO15tvNxtj2xINnD/Q5CZl8/Nt+qmqa74NKiEudBLq9xd4Awb2NudMrS+1WRoi3K+9M7U1VjYUb3lvH1oxC/jKqE1P7hfPSdbFszyxi8a4jJGUXk3zEGJ2z7WBRg+eyWDT7co9RXmUhKbu4Od+GEJc0qwJdKTVWKZWslEpVSj3VwP5HlVK7lFLblVJLlVLN1yHc0plMxgpHJdmw9r92LSXSz51/TehGVuFxOgV6cn2fUACu7RVClJ87by/dy9zNmTiZTQyJ9jvrePZDxeWUVRrL723aX3De1z1UdJySctvcaCXEpey8ga6UMgPTgXFAV2CqUqrraYdtBeK11j2AucArti60VWs/ELpONIYxJs60aymT+oTy0rWxvHNTL8wm44Kn2aT404iO7NzfC7MAACAASURBVDpUzJfr07miSwDDYvzJKjxObknFGefYW9uCNymjL/68r/nuOl5auNu2b0SIS5A1LfR+QKrWOk1rXQnMAibWPUBrvVxrXVb7cD0QatsyLwHj3zDmd/n+jzZfYLoxlFLc1D+cmEDPetsnxgUT3taNaovmut6h9AzzBmBbA6301JxjAAzvFEBC+tFzTuObd6yCrMLjrE87f/ALIc7NmkAPAeouMZ9Zu+1s7gR+bmiHUuoepVSCUiohNzfX+iovBe6+cMv3MPAB2PQhbJxh74rqcTCb+Pv4LgyJ9mN4J3+6B3thNim2ZTYc6G3dnRjTLZCC0krS8s5+bSDlsNGa359XSkFp5VmPE0KcnzWB3tBA4wabXEqpm4F44NWG9mutZ2it47XW8f7+/tZXeakwO8DoFyB6tHGR9Gi6vSuqZ3S3IL64sz+OZhOuTmY6BXo22I+emnOMjv4exEe0Bc7dj77n8KnpD841akYIcX7WBHomEFbncSiQffpBSqmRwNPABK31mR2rwjpKGdPrKhP89GebL11nSz3DvNl2sJDi8io+XJ3GgbxStNbszTlGhwAPovzcaevuxKYDRyk6XkVqzplz16QcKaGNiwNmk2JLukwaJsTvYU2gbwKilVKRSiknYAowv+4BSqlewPsYYZ5j+zIvMd5hcMU/Yd8yWPKPZr3pqDHiwrwoLq9m2CvLeWHBbl5YsIu8Y5UUHa8iOsADpRTx7X2Yvy2LXs8tZuTrq1i7L6/eOfYcLqFbsBdd2nnKikpC/E7nDXStdTXwALAI2A3M0VonKaWeU0pNqD3sVcAD+EYplaiUmn+W0wlr9b3L+Fn7jnEnadVxe1d0hr4RbTGbFGFt3ZjQM5ile3JYtse407RjgAcAU/uH0z/SlwcujybE25V/zd9Fde3NRhaLZu+REjoFedIrzIdtBwupsVy830iEuNhZtWKR1nohsPC0bc/U+e+RNq5LmEzG3Ok+EbD478ZUu1O/Bgdne1d2UpS/B789eTkBns7klFSwcMchXl2UAkB0oBHoIzoFMKJTAABd23ly35dbmLkxg1sHRpBVeJzSyho6BXni4mjii/XppBwpoUu7NnZ7T0K0ZHKn6MVMKbjsQZjwDuxbCt/eCTUX18yIQV4umEyKIC8XxnYPIu9YBR7ODgS1cTnj2DHdgrisgy//WZxCQWklybUXRGMCPekdbszlLt0uQlw4CfSWoPetxvJ1u3+EL6+FwzvsXVGDbrssAoAO/u4NzsKolOKfV3fjWEU1/1mcfHIKgZhAD8LbutHW3YnN6RLoQlwoCfSWYsAf4ao34dB2eG8ILHr6ohsB06e9D8M7+TO8toulIZ2CPLllQHtmbsxgwfZDhHi74uniiFKKYTH+/LT9EDuzjDlitmcW8uavKQ32q7+/ch+frztAeZV9bsAS4mIk86G3NMePwpJnYMvnRh97v7vtXVGjFZVVMeI/KygoreSKzgF8dFtfAPKPVTD+7TW4OJp4alwXHp2TSFllDd/+cSB92rc9+fyDBWUMeWU5AH4eTjw/sTvjYtvZ5b0I0dxkPvTWxNUHrnoLYsbBL0/Bgd/sXVGjebk58viYToDRYj/B18OZd27qxcGjx7nvy82EeLtiUrAypf5QxwU7DgHw1pQ4fN2d+dePu7DI6BghJNBbJJMJrnvfGAHz9RT47W2oKrd3VY0yOT6MR0fFcEN8WL3tfSPa8tzEbozuGsg39w0kLsyblSn1p4lYuOMQPUK9mBgXwp8u78jh4nI2NjAJ2NaMozw7P4kxb6xic7rMFSNaPwn0lsrFC26eB+EDjJuP/tf/or1Y2hCzSfHQFdFE+rmfsW9a//bMuDUebzcnhsUEsD2zkKO187xk5JexPbOI8bVdLCO7BODmZOaHxPo3L6/em8u1/1vLzI0ZZBUe58UFu885SZgQrYEEekvm0x6mfWNM6lVTBR+PhZRF9q7KpobG+KE1rE41ul1OdLdcWRvobk4OjOoayMIdh6isPnVH7YLth/B0dmDT0yP565Wd2ZJRyIoUmRBOtG4S6K1BhxFw11JoGwUzb4TPrjZmbLTjCki20iPUG283R1bVhvHCHYfoGeZNWFu3k8dM6BlM0fEqVu81jtFaszw5hyExfni5OnJDnzBCfVx5Y0mKtNJFqyaB3lq0aQe3/wzDnoDiQ8ac6h+NhsIMe1f2u5hNisEd/ViRnMsDM7ewI6uIq3vUH9EyJNofbzfHk90uuw4Vc6S44uTwSScHEw9dEc32zCLu/nwzf/tuBz8kZsmFVNHqSKC3Js4eMOJv8MAmuOkbKDwIM0bAjrlQ3XInwBwa40/esQqW7DrCIyNj+EPtDUwnODmYmNAzmJ93HmJ/Xikrko2W+vBOp6Zovq5XCFf3DGZvTgkLdxzi4VmJXP3fNbyxJIV7v0jgoa+3UizL4IkWTsaht2Z5e2HWNMhLNoY79rgRet1sXFBd9z/I3QM3fml8EFzEyqtq+HTtAcbHtqvX1VJXTkk5I15dwcAOvhSWVVFRbeHHBwc3eKzFovlxezav/JJMVuFxIv3cyTxaRo9Qbz6/ox/uzlZNcWRTWzOOEhvihYNZ2lji3M41Dl0CvbWz1EDaCtj6BexZADWVgAKTGSzVMOo5GPSwvau0ifdW7uPfP+8B4KEronl0VMw5j6+usVBRbcHd2YGfdxziTzO30D/Sl8/u6IeTQ/MFa2pOCSNfX8VfRsXw4BXRzfa6omWSG4suZSYzdLwCbvgU/pIM416BYU/Cw9sgaoQxhr0VXDwFuH1QBO19jRb8iE7nXxHLwWw62RofF9uO127oybq0fN74NaXecaUV1SQeLOTD1Wnc8ekmXlpo2yGQG/cb89d89Nt+SisursnXRMvS/N8thf24tYX+9556PPwp+HgMJHxszOrYwjk7mHnl+h58szmTHqHejX7+db1D2XSggPdW7mN4jD9VNZrnfkoi5cixk8cEeDqzbE8OHf09mNw37BxnM7qKHM0mzKb6E5VtSMtnZ3YxdwyKQCnFloyjODmYKCyrYuaGDO4eGtXo2q11uKicIK8zZ8IUrYN0uVzqPp8IR5LgTxuNwL/ElVZUM/7t1eQdq+RYRTWRfu5c3zuEjgEe9Aj1JrCNC7d+vIHN6UeZ/8BgYgI9GzxPZbWFEa+tYGz3IP5xVVcAjpZW8vyCXczbkgXATw8OpnuIF5e/toIofw/KKqvZm3OM1U+MwMXRbNP3VVlt4dkfk5i5IYPHRsfwwOXStdNSSZeLOLsRfzcm/Hr3Mti7xAj3nfPg8E57V2YX7s4OvDmlFy6OJu4dGsXPDw/hgcujGdu9HcHerphNijdujMPD2YFrp//GkFeWcctHGygsq6x3nmV7jpBVeJyvNqSfvMv1oVlb+XFbNncMisSkYPGuIxSUVpKWV0rv9t48MKIjuSUV/G95qk3fU9HxKm7+aAMzN2TQOciT1xanMCfhYKPPc6msJvVDYtYZSyW2FBLol7qwvnDXr+DcBr6aZAT73NvhvUHw7mDY/o29K2x2cWHeJPx9FH+9skuDLeUATxc+vb0fE+JC6B3uw7p9+fxzflK9Y2ZvOoiXqyPlVRa+2pDO6r25rN6bx1PjuvDM1V2Jj2jL4qTDbK1d0KN3uA8DO/gyMS6Yt5el8tLC3VRU17D7UDFZhWcuP2ixaHJKyq3qy/9qQzob9xfw1pQ45j8wmCHRfvx13o5GzW+TU1xOnxeW8K8fky4o2OdtySTzaFmjnnOwoIyqmqZfT3fP4eJ67+n5n3bz1q97m/x1m4L0oQsI7gX3roRtX4OTJ/h1hMwE2PIZzLsLDqyCnlNh7X8hf68x1r3rNcaKSpeo7iFevHxdLACRfu68+etexnUPYmz3dhwqOs7KlFzuH96R7VlFfLYuHf8dhwn1ceXmAeEAjO4ayAsLdvNDYjZmk6JnqDdKKV6fHIeXqyMzVqXxweo0tAY/D2dWPD4cD2cHCssqeX1JCouTjnC4uJwb48N48dru5xzuuDI5l67t2jAxLgSAd2/uQ/wLS1iw/fDJaYmzC4+zcX8Bh4vLCWzjzDVxIfUWKVmRnEthWRWf/HaAI8XlvD45zupuodScEh6ds43reoXw+o1xVj0n71gFV7y+kn+M78ItAyOses6FSDxYyDXTf+Odqb24uvaO47xjFVRU1WCxaEymlvV3XAJdGBxdIf6OU4+DexmPl78Eq18z5l938QbPdvDNbRA1HHwiwewIkUOh4yhwtMHFtuJsWPUqVByD62a0iA+NP43oyK+7j/D0dzvx93RmbWo+Fm3MKNkvsi23fryR3JIK3rwxDmcHIwRHdw3ihQW7mb8tm9gQL1ydjO1mk+JfE7rRPdiLA/mltHV34oUFu5mxKo1HRkbzxNztLNuTw8gugfi4O/H1xgzySyt4Z2rvk+eo61hFNZvTj3LXkFMXWj2cHege7MW2zMJT72HmFrZmnHq8YPth/nNDT7zcHAFYtTeXAE9n7h4SxYsLd1Nj2cq70/pYFXjza+/g/SXpMC9UVuPmdP7Y2ZBWQGW1hR21i5001vbMQsJ83PBxdzrncXM3H6S3SiEpK5yrewaTlmtcAC+pqCajoIyIBiaPu5hJoIuzM5nhin9AxCDjJqW4m8DBFTa8C+vfM/rbq47Dxhng7AXRI41g7zTWuJHpBK3hwGpIWwldrobgBlpplaWw6jVY/z+orp0KOHYSxIxpnvf6OziaTbw+OY6bPljP9e+uw8ls4rIOvoT7uhHW1pVuwW0wKcWEnsEnnxPu60bnIE/2HC6hT3ufeudTStUbQbP1YCEfrErD3cnM4l1H+NuVnblnaAfAWHj7mflJTPtwPR/9oe8ZAbZuXz7VFs3QaL962+PCvPlifTpVNRbKq2rYdrCQOwdH8vDIaOYmZPLyz7uZMH0NCx8agoujmTWpeVzROZC7h0ZhMime/2kXby3dyyPnGeuvteb7xOyTC4kv2XXk5DeFc1mflg9Aas6x8xx5pvKqGia/v44rugQy/abeZz2ustrCgW2rmOf8LJ/uKwJiScs9NYR3Z3aRBLpohTpcbvyccNmDp4Y51lTD/pXGhdS9i2Hnt+DkYbTuo4ZBXirs+gEy1hrHr34N2sVBm2BQJvAINFr9Wz6HogyInWyMk//iWiPgo0dDxnr4+Qm4+i0IOfs/UHuKCfRk1RMj+GJdOl9vzOCe2qGHSim+vmcACjBZKqGy3LhTFxjdLYg9h0voFX7uIZZPjOnE4qTDvPzzHuLb+3Dn4FOt7VsGRuDv6cxDsxKZ9N5ahsb48/OOw4T7uvH13QNYlZKLq6OZPhH1PzR6hnnz4Zr97DlUQt6xCiwarugSQBsXR+4YHEnHAA9u/XgjszcdpE97HwrLqhgaY3wo3DEogt2Hinlr6V7a+7pxXe/Qs9aeeLCQjIIyXrm+B2/+msL3W7MaHeha6wbXqD2bLelHKa+ysGjnYXKKywloYMFygOXJOQyt+g0cIOboKgD25R5jiDmJIjzYmdWBq3oEn/nE6gpYN934O+7a+OGxTcmqi6JKqbFKqWSlVKpS6qkG9g9VSm1RSlUrpSbZvkxx0TI7GDcuXTPduHHprmUQMxbW/Re+vB5+edKYIGzcq/DYXhj7f+DgDEUHIX+fMc/M8heM6Qdu/xmu/8Dowx/0EGRuhMSvYPY0OLzduFhbfmFfwZuDm5MD9w7rwIrHR9RbV7WNiyOeLo7Gh9J7g427d4Eb+oQysksAw2LOfRNUe1937hgciaezA6/e0POMce1ju7fj8zv6kVNSwVfrMwhr68rG/QV8vu4Aq/fmMrCD78munhPiwowgSswsZF1aPk5mE73DT4X+0Bh/+kW05aM1+1m2JweAQR2NQFdK8eK13ekb4cOjc7bxp5lbyC05NVeQ1prswuNorfkhMRsnBxNjY4OY2CuEVXvzjD7q6hrSco+xOb2A1JySerXlHatgb84xgr1cKC6vrndua6xPy8ekoNqimbXp7KN5vtucyXiHTQD0qdlOaUkRWUdyed/xP8xwfpPkrLNMt5z0PSz9F6z+T6PqOqk4+/zHXKDzttCVUmZgOjAKyAQ2KaXma6131TksA7gNeKwpihQthMkEoX1g0kdGV03hQfCLAY+AU33hA+4zfuoqLzIuxprqtC963QwrX4Ef/mT03V/zLvzwAPz4sHG367Ec8A4HlzbN9/5+j+oK41tMRTGkr4XIIYR5aD6MXg/JKeAXDaF9z3rN4KmxnXnw8mg8zjLPzIAoXzb87QpqLBoPZwdu+2QT//55DxXVFm47bTIzgFAfV/w8nEjMKGRvTglx4d5nXOS8d1gUd36WwIxVaXQPaYOfh/PJfc4OZr66awAzVu3j7aWp7M4uZvEjQ3Ewm/g+MYtHZm8jys/duLjZ2Wj5X9srhHdX7GPaBxvIKCjjeO0C304OJhb/eejJ7o2N+43RN1P7hfOfJSmk5hyr18renH6U1XtzubFvGO28XAGoqK45+aG1Pq2A2BAv2rg6MnOD8W3p7aV7ySgo418TuuHr4UxBaSWHkjcS4phDdvjVBGf8SHbiL0Qd2oob5bhZyumcNQ+tB5/57WDnt8bvTR/B4Ecad/9GxgajoTPqWeh7l/XPs5I1XS79gFStdRqAUmoWMBE4Geha6wO1+5p+jJFoGXwijB9r1HZB1OPoCkP+YqzGNPkz4yJsyWGjZZT0nXGM2dn4dhDW3/jq6+pjhL+TB1iqjHME9TgVkhXHwNGt/geHtcqLjOeaHRveX3UcsrZAaLzxDeR0+5YZYQ5GIEQOMb62L3/x1DEj/g7DHm/w9Eqps4b5CXUvNr5wTXdGv2F0Iwxt4BuAUsbImnX78jhcXH7qRqOq48bcPwVpjMBETEAXUnJKGRJdew6tYfOn0H4QTv7GDUod/D3441dbWLjzMFfFtuO/y1KJ8HUjoI0z+/NLmdLPGNkTE+jJ0Bh/MvJLmRwfSs8wbzycHfjLnG08Mz+Jz27vi1KK9Wn5uDmZubZ3iBHouce4rPbbgdaaDV+/wKCyVYxb9iSd2odwIL+UvGOVzLl3AF3bebHtYD6vdUohyseBf6TWMP6NGvYVVOFgUiQcOMq0/uF8ti6d29QGtDJTdfm/KP5kKTp5IWOOb+Gwewy4eHFn/lwO5f6N4IA6f35lBbBvqfEtNOUXWP8uXP70qf2Zm2H/CuNbWEAX45rRCRnrjTD3CIROV57z/+WFsibQQ4C631sygf4X8mJKqXuAewDCw8Mv5BTiUjLgPug1DZxr78Yc9GejtV9ZBu6+xtDKXT9A8sKzn6PfPTD237DnJ/juj8YqT1c8Y5xz29dgcoTL/w7ufsaC2zvnGi3/wFiIGGyM3En+BebdbfwDvfUH44MibSUk/2wcW1UGG96H0hxo28H4BhE9sn4dO+cZHzaRQ42aRz0HG94zLiKP+z/49VlY9Qp0nQD+neo/t7oCUpdC+m9GV1VlGcRNhS4TjS6vBoS1deO5id1YkZzb4DJ/YHS7LK3tThkY5QsWC8ycDPuNDwIT8M+BM5iW48HlnQOMMP/lKaPu4F5w93JQijHdgugY4MG7K/bh7GBiX24pb02JY2JcCJXVlnoTnX1+R78z6nh09HFe/nEbWxd9QW/HdIbt3MTR0PsI8XbFw9mh3oXRVbsymXJ8Fm1Nx/gq8CueKH+EK8NrCEj7ntU/JGEaOIDPzc/TP82YpG2eMySWdiR90hw6tPPjhS8WkLfsB3oHj+LOyp2otoMIDY/iZx3HmKwFOKoqNnV4Bs+I3nT+8Rr2L/03THntVKNg94/GpHbD/2p8uG94Hy57wGiU7FsGM6dATZ0uomvfh55TKNm9DKdvpuHg3Q7zbQuM9QuagDWB3tB3wAu6ZUxrPQOYAcat/xdyDnGJca5za73JZHTFnND9ehjzkjFCprwQjhcavytLweQAqb8ao2bS18KRnUYIlRcbC2uD0c1TXQ6750P4QCP0HVyhuvZGHhcvaD/I+MDw7QgHN8K3dxkrRC18wvhHbqmdTCtqBHT9G6x9B7663miBjXkJ2kYard7khdDtWug0zni9efdAWb7xLcS3A4x/3RgJNP9BuP4jSFtujCIqSDO+plcUGbV5hRqBMfcO8G4PE94xLj434Ib4sDMW4a6rZ20/upODybgwu/kTI8xHPWdcnJ4xnMuOzGTJI18SHeBhfOhseA9C+kDWZuM9dR6PyaS4b1gHds37N/2++YGP3bszTN8KVdfidPpQ1vx9xofbiW9ZA+7jVt8ARrv9lZD12dRgYrjWtLN4o9SVdAjwqBfoOxd/yjB1DEun8XRLXsCCWF/jA9dSAgXAAihRrhwf/19cOw4hP3EBcSv/Rlz2m9D+QWaa/4nJMQed9yVK18Cg+zCbFLs8L+Oq0rWUaFdMPSbTvn0wP3x/GROTP4RPd8Po59HBvSnaNAvlGs47mx25LfYBQnf/iOW9oWxyG0rvw7Ox+EThfPt84+/tVzcY/z+PHsB15WvsqwlkU5d3ubmJwhysC/RMoO7filCg6Xr1hWgMpYwLqs4eRtjV1fEKY1m+n5+AuJvhqteNkTU75xm/O4+Howfgh/uNETpDH4fBjxqBmbkZts82vlbHTYPx/zFG4vz8hBH8HUfBDZ8YrefKY6e6l+JuMj5EVr4K0/tDv7uNUK88ZgR6xGBjiGfKzxA2ANoPNJ7n4W98k/juXnizu7HNydN4bteroeu1RnCbHY2WdPJCo/vpi2vg8n9A9Chjpao2wRDYzarx+ycCvVeYNy6lWbDkGYgcBpc9ZDx/wH2oX58l2pIGG9bCb28aIzvGvWK8t2UvQsw4MJmY2K6QCY5fs98SxACnFMw/3AdL/2lMBufqAyVHjD/LQ4nGi4dfBpUlMP9BzICfVyT/Z3qW5ZVdub38CyYf+Qny9tLVz4Ghe16A30aSGHYrg45+R6FHJN43fgmzpsKObyBqOOWjX+P+DxYRVp5CduBwPuh7HQC+I/4ElnzjAubOeZgc3WDat6gDq4z/x92uBaAwZDgVyW/xbc0Qrm4XgKuTmfd8nqDQYTl/yP0KPricbK/eBBVuZXrNRD5Zl87Xm0y83etNApI+oH/h56RYQpia9RADfsjknam9ME3+HD64HFa8zDZLDHdUPkbQzgqmjWzcqJ3GOO/kXEopByAFuALIAjYBN2mtkxo49lPgJ6313PO9sEzOJZpNRUn9lv7pLBajf9uaIWi/vW3MfTPi6bN2dwDGSIalzxkfCtoCrm3hsRQjkL+/3xi9M3W2MWb/BK2N4DE7GQHt3/ncwVxxzGgBJs2rv90rzGj1H8sFtDFMtG0UFB6Agv1GF5G2QKfxvFgwnBE+eVy2+0Xjw+3+dUa3FBjfeN7oZnxYHUmCLlfBDZ8b35R2zIVv7zTCvdfN8MmVlOdncJvrO3xy/xhcM9fAmtdPdt8Axjek7tcbIeoVarzftOVQlAU9Jp+69nAsF97qCR0vJ+NIPuEFxpDXBLfBxJetoWLUyzgPut/4JpaZYHRjKcVHa/bz/E+7uHdYFH8d1+XU61pqjG9lh3fArfPB/8yx8/9bkcq3i5ZxzLkd6/95FUopXvhpFx/9tp9Zt3SlU/Y8Slf/jyBVQMntqyjz6sjDs7ay6cBR2rg4MGO8D9FRkXywMZ/3Vu7jmau6csfgSMjfxw9f/Zfn8oZzy9AuvPnr3pOTsl2o373AhVLqSuBNwAx8rLV+USn1HJCgtZ6vlOoLfAf4AOXAYa11t3OdUwJdXBIK9hvdFIHdoPetxrb8fUaXw+BHL+wCbV1aG98uqo4b4/nzkiFlkXEB2TMIaqogewuU5hoX49pGGReNq8qMPnknT6Ol7BlsjPOPGV3//IueNoagBveG2xaAU+2KURYLfDDCaHGbHIyup8mfQ9eJ9Z9feNC4Qc3Vx7j2YK2lzxv3LABPVd3F1UGFDMqfS5XZFcfHkxu8kF5eVcNzP+3izsGRdPA/bRUui8W4UN7QBWvg111HuOvzBHqHezPv/kEAlFVWc9U7aygpr6ZzkCeJ6XksubsTQWEdAWOBlO+2ZjEgyvfkSlpaa+7+PIFVKXl88Id4dmQW8triFB4dFcMfBkbQ96VfualfOM9OOGc8npOsWCTEpUxrI/CdTlu+L3urccevT3tj1SqnBi6eluYZXS0DHwTPwPr7KsuMETH7loG7Pwx/0nY1Hy+EL68nv+N19FkUAWj+EfAbt4/ojqnXNNu9Tq2DBWUMeWU5k/qE8toNPU9u332omInTf6Oy2sJT4zpz37AO5z1X/rEKxr61+uT4+YFRvnx0WzxuTg78aeYW1qbmseFvIy94VSwJdCFEi1Rj0XR55heczSZ+/vMQQn0aXlP297JYNDe8v47bB0WccXfoD4lZLN51hDcmx1kdwtsOFrImNY9x3YOIqvNtYXlyDrd/son3bu7D2O5BF1SrBLoQosX6aM1+ogM8GhxP39JU11h4eHYiN/dvz8AOvhd0Dgl0IYRoJWTFIiGEuARIoAshRCshgS6EEK2EBLoQQrQSEuhCCNFKSKALIUQrIYEuhBCthAS6EEK0Ena7sUgplQukX+DT/YA8G5bTlFpSrdCy6pVam4bU2jRsVWt7rXWDt83aLdB/D6VUwtnulLrYtKRaoWXVK7U2Dam1aTRHrdLlIoQQrYQEuhBCtBItNdBn2LuARmhJtULLqldqbRpSa9No8lpbZB+6EEKIM7XUFroQQojTSKALIUQr0eICXSk1VimVrJRKVUo9Ze966lJKhSmlliuldiulkpRSD9dub6uUWqKU2lv728fetZ6glDIrpbYqpX6qfRyplNpQW+tspZSTvWsEUEp5K6XmKqX+v32zCY2riuL470+jkVak1lKNTSGtBLUWtcFFoi7ED9qUEhFcpBQMWHAj+IGiDgHBpShahVoFPwISKlirhoIWia6rVto02EYrDTa1mi60gm5aPC7uGfsyeaOCTu6d4f7gK5PONQAAA9lJREFUMe+eewf+/OedM2/OfXPU/e1L1VdJj/rnPylpl6SLUvJV0puSZiVNFmKlXirwsufbhKSeBLQ+59fBhKT3JS0tzFVc65SkDbG1FuYel2SSlvu4Ib42VUGXtAjYAfQDa4EtktbGVTWHc8BjZnYt0As86PqeAsbNrBsY93EqPAwcKYyfBV50rT8D26Koms9LwMdmdg1wA0Fzcr5KWgk8BNxkZuuARcAgafk6AmysidXzsh/o9uMBYOcCaawywnytnwDrzOx64BugAuC5Nghc5+95xWvGQjHCfK1IWgXcBXxfCDfGVzNrmgPoA/YVxhWgElvX3+j90D/IKaDDYx3AVGxtrqWTkLy3A3sBEf7J1lbmd0SdlwDH8U38Qjw5X4GVwAlgGdDmvm5IzVegC5j8Jy+B14AtZetiaa2ZuwcY9fM59QDYB/TF1grsJtyETAPLG+lrU92hcz5Zqsx4LDkkdQHrgf3A5WZ2CsBfV8RTNoftwBPAHz6+DPjFzM75OBV/1wCngbe8PfS6pCUk6KuZnQSeJ9yNnQLOAAdI09ci9bxMPefuBz7y8+S0ShoATprZoZqphmhttoKuklhyz11Kuhh4D3jEzH6NracMSZuBWTM7UAyXLE3B3zagB9hpZuuB30igvVKG957vBlYDVwJLCD+va0nB139DqtcEkoYJbc7RaqhkWTStkhYDw8DTZdMlsf+stdkK+gywqjDuBH6IpKUUSRcQivmome3x8E+SOny+A5iNpa/ALcCApGngHULbZTuwVFKbr0nF3xlgxsz2+3g3ocCn6OudwHEzO21mZ4E9wM2k6WuRel4mmXOShoDNwFbzngXpab2K8MV+yPOsE/hK0hU0SGuzFfQvgG5/YuBCwgbIWGRNfyFJwBvAETN7oTA1Bgz5+RChtx4VM6uYWaeZdRF8/NTMtgKfAff6slS0/gickHS1h+4AviZBXwmtll5Ji/16qGpNztca6nk5BtznT2X0AmeqrZlYSNoIPAkMmNnvhakxYFBSu6TVhA3Hz2NoBDCzw2a2wsy6PM9mgB6/nhvj60JuGPxPmw6bCDvb3wHDsfXUaLuV8LNpAjjoxyZCb3oc+NZfl8XWWqP7NmCvn68hJMEx4F2gPbY+13Uj8KV7+wFwaaq+As8AR4FJ4G2gPSVfgV2E/v5ZLzLb6nlJaA3s8Hw7THh6J7bWY4T+czXHXi2sH3atU0B/bK0189Oc3xRtiK/5r/+ZTCbTIjRbyyWTyWQydcgFPZPJZFqEXNAzmUymRcgFPZPJZFqEXNAzmUymRcgFPZPJZFqEXNAzmUymRfgTkTu5h0hbw/kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_loss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54,  1],\n",
       "       [ 4, 84]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96        55\n",
      "           1       0.99      0.95      0.97        88\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.96      0.97      0.96       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
